{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All modules successfully imported.\n"
     ]
    }
   ],
   "source": [
    "# Import modules\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plot\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.linear_model import ARDRegression, BayesianRidge\n",
    "from collections import Counter\n",
    "print('All modules successfully imported.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define user input variables\n",
    "root_folder = 'E:/VegetationEcology/Data_Harmonization/Project_GIS/Data_Output/testRegression'\n",
    "output_folder = 'E:/VegetationEcology/Data_Harmonization/Project_GIS/Data_Output/testRegression'\n",
    "file_10 = 'E:/VegetationEcology/Data_Harmonization/Project_GIS/Data_Output/testRegression/betula_nana_10.csv'\n",
    "file_25 = 'E:/VegetationEcology/Data_Harmonization/Project_GIS/Data_Output/testRegression/betula_nana_25.csv'\n",
    "file_50 = 'E:/VegetationEcology/Data_Harmonization/Project_GIS/Data_Output/testRegression/betula_nana_50.csv'\n",
    "file_100 = 'E:/VegetationEcology/Data_Harmonization/Project_GIS/Data_Output/testRegression/betula_nana_100.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable sets loaded.\n"
     ]
    }
   ],
   "source": [
    "# Define variable sets\n",
    "predictor_metrics = ['compoundTopographic', 'dateFreeze_2000s', 'dateThaw_2000s', 'elevation', 'floodplainsDist', 'growingSeason_2000s', 'heatLoad', 'integratedMoisture', 'precipAnnual_2000s', 'roughness', 'siteExposure', 'slope', 'streamLargeDist', 'streamSmallDist', 'summerWarmth_2000s', 'surfaceArea', 'surfaceRelief', 'aspect', 'may_2_blue', 'may_evi2', 'may_nbr', 'may_ndmi', 'may_ndsi', 'may_ndvi', 'may_ndwi', 'june_2_blue', 'june_evi2', 'june_nbr', 'june_ndmi', 'june_ndsi', 'june_ndvi', 'june_ndwi', 'july_2_blue', 'july_evi2', 'july_nbr', 'july_ndmi', 'july_ndsi', 'july_ndvi', 'july_ndwi', 'august_2_blue', 'august_evi2', 'august_nbr', 'august_ndmi', 'august_ndsi', 'august_ndvi', 'august_ndwi', 'september_2_blue', 'september_evi2', 'september_nbr', 'september_ndmi', 'september_ndsi', 'september_ndvi', 'september_ndwi']\n",
    "zero_variable = ['zero']\n",
    "cover = ['cover']\n",
    "coverLog = ['coverLog']\n",
    "strata = ['strata']\n",
    "retain_variables = ['project', 'siteID', 'siteCode', 'methodSurvey', 'methodCover']\n",
    "coordinates = ['POINT_X', 'POINT_Y']\n",
    "all_variables = retain_variables + coordinates + predictor_metrics + zero_variable + strata + cover + coverLog\n",
    "scale_variables = predictor_metrics\n",
    "print('Variable sets loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import input data from csv file\n",
    "data_10 = pd.read_csv(file_10)\n",
    "data_25 = pd.read_csv(file_25)\n",
    "data_50 = pd.read_csv(file_50)\n",
    "data_100 = pd.read_csv(file_100)\n",
    "# Convert numerical data to integers\n",
    "data_10[predictor_metrics + cover + coverLog] = data_10[predictor_metrics + cover + coverLog].astype(float)\n",
    "data_25[predictor_metrics + cover + coverLog] = data_25[predictor_metrics + cover + coverLog].astype(float)\n",
    "data_50[predictor_metrics + cover + coverLog] = data_50[predictor_metrics + cover + coverLog].astype(float)\n",
    "data_100[predictor_metrics + cover + coverLog] = data_100[predictor_metrics + cover + coverLog].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    }
   ],
   "source": [
    "# Determine sample size for each class\n",
    "n_10 = len(data_10[cover])\n",
    "n_25 = len(data_25[cover])\n",
    "n_50 = len(data_50[cover])\n",
    "n_100 = len(data_100[cover])\n",
    "# Determine minimum sample size for a 70% training ratio\n",
    "n_min = int(min([n_10*0.7, n_25*0.7, n_50*0.7, n_100*0.7]))\n",
    "print(n_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06 0.14 0.29 0.69\n"
     ]
    }
   ],
   "source": [
    "# Determine sampling ratio for each class\n",
    "p_10 = round(n_min/n_10, 2)\n",
    "p_25 = round(n_min/n_25, 2)\n",
    "p_50 = round(n_min/n_50, 2)\n",
    "p_100 = round(n_min/n_100, 2)\n",
    "print(p_10, p_25, p_50, p_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and test splits for the 1-10% data\n",
    "X = data_10[all_variables]\n",
    "y = data_10[coverLog[0]]\n",
    "all_train_10, all_test_10, y_train_10, y_test_10 = train_test_split(X, y, test_size = 1-p_10, train_size = p_10, random_state = None, shuffle = True, stratify = None)\n",
    "# Create train and test splits for the 11-25% data\n",
    "X = data_25[all_variables]\n",
    "y = data_25[coverLog[0]]\n",
    "all_train_25, all_test_25, y_train_25, y_test_25 = train_test_split(X, y, test_size=1-p_25, train_size = p_25, random_state = None, shuffle = True, stratify = None)\n",
    "# Create train and test splits for the 26-50% data\n",
    "X = data_50[all_variables]\n",
    "y = data_50[coverLog[0]]\n",
    "all_train_50, all_test_50, y_train_50, y_test_50 = train_test_split(X, y, test_size=1-p_50, train_size = p_50, random_state = None, shuffle = True, stratify = None)\n",
    "# Create train and test splits for the 51-100% data\n",
    "X = data_100[all_variables]\n",
    "y = data_100[coverLog[0]]\n",
    "all_train_100, all_test_100, y_train_100, y_test_100 = train_test_split(X, y, test_size=1-p_100, train_size = p_100, random_state = None, shuffle = True, stratify = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build training sample\n",
    "all_train_raw = all_train_10.append(all_train_25, ignore_index = True, sort = True)\n",
    "all_train_raw = all_train_raw.append(all_train_50, ignore_index = True, sort = True)\n",
    "all_train_raw = all_train_raw.append(all_train_100, ignore_index = True, sort = True)\n",
    "\n",
    "# Build test sample\n",
    "all_test_raw = all_test_10.append(all_test_25, ignore_index = True, sort = True)\n",
    "all_test_raw = all_test_raw.append(all_test_50, ignore_index = True, sort = True)\n",
    "all_test_raw = all_test_raw.append(all_test_100, ignore_index = True, sort = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output raw train and test data\n",
    "train_raw = 'E:/VegetationEcology/Data_Harmonization/Project_GIS/Data_Output/testRegression/train_raw.csv'\n",
    "test_raw = 'E:/VegetationEcology/Data_Harmonization/Project_GIS/Data_Output/testRegression/test_raw.csv'\n",
    "all_train_raw.to_csv(train_raw, header=True, index=False, sep=',', encoding='utf-8')\n",
    "all_test_raw.to_csv(test_raw, header=True, index=False, sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a standard scaler to set mean = 0 and scale unit variance (scale all variables to Gaussian distribution)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(all_train_raw[scale_variables])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the training data\n",
    "train_scaled = scaler.transform(all_train_raw[scale_variables])\n",
    "all_train_scaled = all_train_raw\n",
    "all_train_scaled = all_train_scaled.drop(columns=scale_variables)\n",
    "all_train_scaled = pd.concat([all_train_scaled, pd.DataFrame(data=train_scaled, columns=scale_variables)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the test data\n",
    "test_scaled = scaler.transform(all_test_raw[scale_variables])\n",
    "all_test_scaled = all_test_raw\n",
    "all_test_scaled = all_test_scaled.drop(columns=scale_variables)\n",
    "all_test_scaled = pd.concat([all_test_scaled, pd.DataFrame(data=test_scaled, columns=scale_variables)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output scaled train and test data\n",
    "train_scaled = 'E:/VegetationEcology/Data_Harmonization/Project_GIS/Data_Output/testRegression/train_scaled.csv'\n",
    "test_scaled = 'E:/VegetationEcology/Data_Harmonization/Project_GIS/Data_Output/testRegression/test_scaled.csv'\n",
    "all_train_scaled.to_csv(train_scaled, header=True, index=False, sep=',', encoding='utf-8')\n",
    "all_test_scaled.to_csv(test_scaled, header=True, index=False, sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate performance metrics based on a specified threshold value\n",
    "def thresholdMetrics(inIndex, inProbability, inValue, y_test):\n",
    "    outThresholded = np.zeros(inIndex.shape)\n",
    "    outThresholded[inIndex > inValue] = 1\n",
    "    confusion_test = confusion_matrix(y_test, outThresholded)\n",
    "    true_negative = confusion_test[0,0]\n",
    "    false_negative = confusion_test[1,0]\n",
    "    true_positive = confusion_test[1,1]\n",
    "    false_positive = confusion_test[0,1]\n",
    "    outSensitivity = true_positive / (true_positive + false_negative)\n",
    "    outSpecificity = true_negative / (true_negative + false_positive)\n",
    "    outAUC = roc_auc_score(y_test, inProbability)\n",
    "    outAccuracy = (true_negative + true_positive) / (true_negative + false_positive + false_negative + true_positive)\n",
    "    return (outThresholded, outSensitivity, outSpecificity, outAUC, outAccuracy)\n",
    "\n",
    "print('Function \"thresholdMetrics\" loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to fit a classifier using training data and determine performance using the test data\n",
    "def trainTestClassifier(X_train, y_train, X_test, y_test, testData):\n",
    "    # Fit a classifier to the training dataset\n",
    "    classifier = RandomForestClassifier(n_estimators=50, criterion='entropy', max_features='log2', bootstrap=True, oob_score=False, n_jobs=16, class_weight='balanced')\n",
    "    classifier.fit(X_train, y_train)\n",
    "    # Use the random forest classifier to predict probabilities for the test dataset\n",
    "    test_prediction = classifier.predict_proba(X_test)\n",
    "    # Convert the positive class probabilities to a list of probabilities\n",
    "    test_probability = [p[1] for p in test_prediction]\n",
    "    # Convert the postitive class probabilities to an index between 0 and 1000\n",
    "    test_index = [int((p[1] * 1000) + 0.5) for p in test_prediction]\n",
    "    # Iterate through numbers between 0 and 1000 to output a list of sensitivity and specificity values per threshold number\n",
    "    i = 1\n",
    "    test_index = np.asarray(test_index)\n",
    "    sensitivity_list = []\n",
    "    specificity_list = []\n",
    "    while i < 1001:\n",
    "        test_thresholded, sensitivity_test, specificity_test, auc_test, accuracy_test = thresholdMetrics(test_index, test_probability, i, y_test)\n",
    "        sensitivity_list.append(sensitivity_test)\n",
    "        specificity_list.append(specificity_test)\n",
    "        i = i + 1\n",
    "    # Calculate a list of absolute value of difference between sensitivity and specificity and find the optimal threshold\n",
    "    difference_list = [a - b for a, b in zip(sensitivity_list, specificity_list)]\n",
    "    value, threshold = min((value, threshold) for (threshold, value) in enumerate(difference_list) if value >= 0)\n",
    "    # Calculate the prediction index to a binary 0 or 1 output using the optimal threshold\n",
    "    test_thresholded, sensitivity_test, specificity_test, auc_test, accuracy_test = thresholdMetrics(test_index, test_probability, threshold, y_test)\n",
    "    # Concatenate thresholded predictions to test data frame\n",
    "    testData = pd.concat([testData, pd.DataFrame(test_thresholded)], axis=1)\n",
    "    testData = testData.rename(index=int, columns={0: 'classify'})\n",
    "    return [threshold, sensitivity_test, specificity_test, auc_test, accuracy_test, testData]\n",
    "\n",
    "print('Function \"trainTestClassifier\" loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to fit a regressor using training data and determine performance using the test data\n",
    "def trainTestRegressor(X_train, y_train, X_test, y_test, testData):\n",
    "    # Fit a regressor to the training dataset\n",
    "    regressor = RandomForestRegressor(n_estimators=10, criterion='mse', bootstrap=True, oob_score=False, n_jobs=16)\n",
    "    regressor.fit(X_train, y_train)\n",
    "    # Use the regressor to predict values for the test dataset\n",
    "    test_prediction = regressor.predict(X_test)\n",
    "    # Calculate the r^2\n",
    "    r_score = r2_score(y_test, test_prediction, sample_weight=None, multioutput='uniform_average')\n",
    "    # Calculate error\n",
    "    mae = mean_absolute_error(y_test, test_prediction)\n",
    "    mse = mean_squared_error(y_test, test_prediction)\n",
    "    rmse = np.sqrt(mse)\n",
    "    # Concatenate predicted values to test data frame\n",
    "    testData = pd.concat([testData, pd.DataFrame(test_prediction)], axis=1)\n",
    "    testData = testData.rename(index=int, columns={0: 'regress'})\n",
    "    return [r_score, mae, mse, rmse, testData]\n",
    "\n",
    "print('Function \"trainTestRegressor\" loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define random forest regression function\n",
    "def trainTestSVRegressor(X_train, y_train, X_test, y_test, testData, variable):\n",
    "    # Fit a random forest regressor to the training dataset\n",
    "    sv_regress = SVR(kernel='rbf')\n",
    "    sv_regress.fit(X_train, y_train)\n",
    "    # Use the random forest classifier to predict probabilities for the test dataset\n",
    "    test_prediction = sv_regress.predict(X_test)\n",
    "    # Calculate r\n",
    "    r_score = r2_score(y_test, test_prediction, sample_weight=None, multioutput='uniform_average')\n",
    "    # Calculate error\n",
    "    mae = mean_absolute_error(y_test, test_prediction)\n",
    "    mse = mean_squared_error(y_test, test_prediction)\n",
    "    rmse = np.sqrt(mse)\n",
    "    # Concatenate thresholded predictions to test data frame\n",
    "    testData = pd.concat([testData, pd.DataFrame(test_prediction)], axis=1)\n",
    "    testData = testData.rename(index=int, columns={0: variable})\n",
    "    return [r_score, mae, mse, rmse, sv_regress, testData]\n",
    "\n",
    "print('Function \"trainTestModel\" loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import input data from csv file\n",
    "input_file = os.path.join(os.path.join(root_folder, \"speciesData\"), input_data_name)\n",
    "inData = pd.read_csv(input_file)\n",
    "# Convert numerical data to integers\n",
    "inData[predictor_metrics + zero_variable + ten_variable + twentyfive_variable + cover + strata] = input_df[predictor_metrics + zero_variable + ten_variable + twentyfive_variable + cover + strata].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = inData[all_variables]\n",
    "y = inData[zero_variable[0]]\n",
    "stratify = inData[strata[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_list = []\n",
    "sensitivity_list = []\n",
    "specificity_list = []\n",
    "auc_list = []\n",
    "accuracy_list = []\n",
    "r2_list = []\n",
    "mae_list = []\n",
    "mse_list = []\n",
    "rmse_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output = pd.DataFrame(columns=[['index'] + all_variables + ['classify', 'regress']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train, all_test, y_train, y_test = train_test_split(X, y, test_size=0.3, train_size = 0.7, random_state = None, shuffle = True, stratify = stratify)\n",
    "all_train = all_train.reset_index()\n",
    "all_test = all_test.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(all_train[scale_variables])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scaled = scaler.transform(all_train[scale_variables])\n",
    "all_train.drop(labels=scale_variables, axis='columns', inplace=True)\n",
    "all_train = pd.concat([all_train, pd.DataFrame(data=train_scaled, columns=scale_variables)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scaled = scaler.transform(all_test[scale_variables])\n",
    "all_test.drop(labels=scale_variables, axis='columns', inplace=True)\n",
    "all_test = pd.concat([all_test, pd.DataFrame(data=test_scaled, columns=scale_variables)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = all_train[predictor_metrics]\n",
    "y_train = all_train[zero_variable[0]]\n",
    "X_test = all_test[predictor_metrics]\n",
    "y_test = all_test[zero_variable[0]]\n",
    "threshold, sensitivity, specificity, auc, accuracy, all_test = trainTestClassifier(X_train, y_train, X_test, y_test, all_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement naive random oversampling to balance dataset\n",
    "resample_train, resample_strata = RandomOverSampler(random_state=0).fit_sample(all_train, all_train[strata[0]])\n",
    "# Convert resampled data back to data frame\n",
    "all_column_names = np.array(all_train.columns.values)\n",
    "resample_train = pd.DataFrame(data=resample_train, columns=all_column_names)\n",
    "# Print resampled strata summary\n",
    "print(sorted(Counter(resample_strata).items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = resample_train[predictor_metrics]\n",
    "y_train = resample_train[coverLog[0]]\n",
    "X_test = all_test[predictor_metrics]\n",
    "y_test = all_test[coverLog[0]]\n",
    "r_score, mae, mse, rmse, all_test = trainTestRegressor(X_train, y_train, X_test, y_test, all_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_list.append(threshold)\n",
    "sensitivity_list.append(sensitivity)\n",
    "specificity_list.append(specificity)\n",
    "auc_list.append(auc)\n",
    "accuracy_list.append(accuracy)\n",
    "r2_list.append(r_score)\n",
    "mae_list.append(mae)\n",
    "mse_list.append(mse)\n",
    "rmse_list.append(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame(data=inverse_scaled, columns=scale_variables)\n",
    "test.to_csv('K:/VegetationEcology/test.csv', header=True, index=False, sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_test = all_test.drop(coverLog[0], axis=1)\n",
    "#all_test = all_test.rename(index=str, columns={'regress': 'coverLog'})\n",
    "#inverse_scaled = scaler.inverse_transform(all_test[scale_variables])\n",
    "#all_test.drop(scale_variables, axis=1)\n",
    "all_test = pd.concat([all_test, pd.DataFrame(data=inverse_scaled, columns=scale_variables)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
