{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All modules successfully imported.\n"
     ]
    }
   ],
   "source": [
    "# Import modules\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plot\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n",
    "from sklearn.svm import SVR\n",
    "from collections import Counter\n",
    "from matplotlib.colors import Normalize\n",
    "print('All modules successfully imported.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable sets loaded.\n"
     ]
    }
   ],
   "source": [
    "# Define variable sets\n",
    "predictor_all = ['compoundTopographic', 'dateFreeze_2000s', 'dateThaw_2000s', 'elevation', 'floodplainsDist', 'growingSeason_2000s', 'heatLoad', 'integratedMoisture', 'precipAnnual_2000s', 'roughness', 'siteExposure', 'slope', 'streamLargeDist', 'streamSmallDist', 'summerWarmth_2000s', 'surfaceArea', 'surfaceRelief', 'aspect', 'may_1_ultraBlue', 'may_2_blue', 'may_3_green', 'may_4_red', 'may_5_nearInfrared', 'may_6_shortInfrared1', 'may_7_shortInfrared2', 'may_evi2', 'may_nbr', 'may_ndmi', 'may_ndsi', 'may_ndvi', 'may_ndwi', 'june_1_ultraBlue', 'june_2_blue', 'june_3_green', 'june_4_red', 'june_5_nearInfrared', 'june_6_shortInfrared1', 'june_7_shortInfrared2', 'june_evi2', 'june_nbr', 'june_ndmi', 'june_ndsi', 'june_ndvi', 'june_ndwi', 'july_1_ultraBlue', 'july_2_blue', 'july_3_green', 'july_4_red', 'july_5_nearInfrared', 'july_6_shortInfrared1', 'july_7_shortInfrared2', 'july_evi2', 'july_nbr', 'july_ndmi', 'july_ndsi', 'july_ndvi', 'july_ndwi', 'august_1_ultraBlue', 'august_2_blue', 'august_3_green', 'august_4_red', 'august_5_nearInfrared', 'august_6_shortInfrared1', 'august_7_shortInfrared2', 'august_evi2', 'august_nbr', 'august_ndmi', 'august_ndsi', 'august_ndvi', 'august_ndwi', 'september_1_ultraBlue', 'september_2_blue', 'september_3_green', 'september_4_red', 'september_5_nearInfrared', 'september_6_shortInfrared1', 'september_7_shortInfrared2', 'september_evi2', 'september_nbr', 'september_ndmi', 'september_ndsi', 'september_ndvi', 'september_ndwi']\n",
    "predictor_metrics = ['compoundTopographic', 'dateFreeze_2000s', 'dateThaw_2000s', 'elevation', 'floodplainsDist', 'growingSeason_2000s', 'heatLoad', 'integratedMoisture', 'precipAnnual_2000s', 'roughness', 'siteExposure', 'slope', 'streamLargeDist', 'streamSmallDist', 'summerWarmth_2000s', 'surfaceArea', 'surfaceRelief', 'aspect', 'may_2_blue', 'may_evi2', 'may_nbr', 'may_ndmi', 'may_ndsi', 'may_ndvi', 'may_ndwi', 'june_2_blue', 'june_evi2', 'june_nbr', 'june_ndmi', 'june_ndsi', 'june_ndvi', 'june_ndwi', 'july_2_blue', 'july_evi2', 'july_nbr', 'july_ndmi', 'july_ndsi', 'july_ndvi', 'july_ndwi', 'august_2_blue', 'august_evi2', 'august_nbr', 'august_ndmi', 'august_ndsi', 'august_ndvi', 'august_ndwi', 'september_2_blue', 'september_evi2', 'september_nbr', 'september_ndmi', 'september_ndsi', 'september_ndvi', 'september_ndwi']\n",
    "predictor_midsummer = ['compoundTopographic', 'dateFreeze_2000s', 'dateThaw_2000s', 'elevation', 'floodplainsDist', 'growingSeason_2000s', 'heatLoad', 'integratedMoisture', 'precipAnnual_2000s', 'roughness', 'siteExposure', 'slope', 'streamLargeDist', 'streamSmallDist', 'summerWarmth_2000s', 'surfaceArea', 'surfaceRelief', 'aspect', 'july_1_ultraBlue', 'july_2_blue', 'july_3_green', 'july_4_red', 'july_5_nearInfrared', 'july_6_shortInfrared1', 'july_7_shortInfrared2', 'july_evi2', 'july_nbr', 'july_ndmi', 'july_ndsi', 'july_ndvi', 'july_ndwi', 'strata']\n",
    "predictor_raw = ['compoundTopographic', 'dateFreeze_2000s', 'dateThaw_2000s', 'elevation', 'floodplainsDist', 'growingSeason_2000s', 'heatLoad', 'integratedMoisture', 'precipAnnual_2000s', 'roughness', 'siteExposure', 'slope', 'streamLargeDist', 'streamSmallDist', 'summerWarmth_2000s', 'surfaceArea', 'surfaceRelief', 'aspect', 'may_1_ultraBlue', 'may_2_blue', 'may_3_green', 'may_4_red', 'may_5_nearInfrared', 'may_6_shortInfrared1', 'may_7_shortInfrared2', 'june_1_ultraBlue', 'june_2_blue', 'june_3_green', 'june_4_red', 'june_5_nearInfrared', 'june_6_shortInfrared1', 'june_7_shortInfrared2', 'july_1_ultraBlue', 'july_2_blue', 'july_3_green', 'july_4_red', 'july_5_nearInfrared', 'july_6_shortInfrared1', 'july_7_shortInfrared2', 'august_1_ultraBlue', 'august_2_blue', 'august_3_green', 'august_4_red', 'august_5_nearInfrared', 'august_6_shortInfrared1', 'august_7_shortInfrared2', 'september_1_ultraBlue', 'september_2_blue', 'september_3_green', 'september_4_red', 'september_5_nearInfrared', 'september_6_shortInfrared1', 'september_7_shortInfrared2']\n",
    "zero_variable = ['zero']\n",
    "strata = ['strata']\n",
    "cover = ['cover']\n",
    "coverLog = ['coverLog']\n",
    "coverLogInt = ['coverLogInt']\n",
    "retain_variables = ['project', 'siteID', 'siteCode', 'methodSurvey', 'methodCover']\n",
    "coordinates = ['POINT_X', 'POINT_Y']\n",
    "all_variables = retain_variables + coordinates + predictor_metrics + zero_variable + strata + cover + coverLog + coverLogInt\n",
    "print('Variable sets loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input file\n",
    "input_file = 'K:/VegetationEcology/Data_Harmonization/Project_GIS/Data_Output/speciesData/betula_nana_quant.csv'\n",
    "# Create data frame of input data\n",
    "input_data = pd.read_csv(input_file)\n",
    "input_data.reset_index()\n",
    "# Convert values to floats\n",
    "input_data[predictor_all + cover + coordinates] = input_data[predictor_all + cover + coordinates].astype(float)\n",
    "# Convert values to integers\n",
    "input_data[strata + zero_variable] = input_data[strata + zero_variable].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate Log2(Cover+1)\n",
    "def calculateLog2(n):\n",
    "    out_log = np.log2(n+1)\n",
    "    return out_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate Log2(Cover+1) rounded to the nearest integer\n",
    "def calculateLog2Int(n):\n",
    "    out_log = np.round(np.log2(n+1), 0).astype(int)\n",
    "    return out_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column of LOG2(Cover+1)\n",
    "input_data[coverLog] = input_data[cover].apply(calculateLog2, axis=0)\n",
    "# Add a column of LOG2(Cover+1) rounded to the nearest integer\n",
    "input_data[coverLogInt] = input_data[cover].apply(calculateLog2Int, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to merge data into another sample class\n",
    "def mergeClass(inData, location, column, inValue):\n",
    "    count = inData.loc[location == inValue, column].count()[0]\n",
    "    if count < 20:\n",
    "        inData.loc[location == inValue, column] = inValue - 1\n",
    "    return inData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge classes that have fewer than 20 observations\n",
    "i = 7\n",
    "while i > 1:\n",
    "    mergeClass(input_data, input_data.coverLogInt, coverLogInt, i)\n",
    "    i=i-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to create and scale the train and test partitions\n",
    "def partitionData(inData, all_variables, predictors, response, strata):\n",
    "    # Create train and test splits\n",
    "    X = inData[all_variables]\n",
    "    y = inData[response[0]]\n",
    "    stratify = inData[strata[0]]\n",
    "    all_train, all_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, train_size = 0.7, random_state = None, shuffle = True, stratify = stratify)\n",
    "    # Reset the index on the train and test splits\n",
    "    all_train = all_train.reset_index()\n",
    "    all_test = all_test.reset_index()\n",
    "    # Create a standard scaler to scale all covariates to Gaussian distribution\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(all_train[predictors])\n",
    "    # Scale the train X data\n",
    "    train_scaled = scaler.transform(all_train[predictor_metrics])\n",
    "    all_train = all_train.drop(columns=predictor_metrics)\n",
    "    all_train = pd.concat([all_train, pd.DataFrame(data=train_scaled, columns=predictor_metrics)], axis=1)\n",
    "    # Scale the test X data\n",
    "    test_scaled = scaler.transform(all_test[predictor_metrics])\n",
    "    all_test = all_test.drop(columns=predictor_metrics)\n",
    "    all_test = pd.concat([all_test, pd.DataFrame(data=test_scaled, columns=predictor_metrics)], axis=1)\n",
    "    return all_train, all_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train, all_test = partitionData(input_data, all_variables, predictor_metrics, cover, coverLogInt)\n",
    "\n",
    "presence_train = all_train[all_train[cover[0]] >= 1]\n",
    "\n",
    "X_train = presence_train[predictor_metrics]\n",
    "y_train = presence_train[cover[0]]\n",
    "X_test = all_test[predictor_metrics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Cover trained...\n"
     ]
    }
   ],
   "source": [
    "# Fit an abundance classifier to the train X and y\n",
    "svrClassifier = SVR(kernel='rbf', C=100, gamma=0.01)\n",
    "svrClassifier.fit(X_train, y_train)\n",
    "# Use the abundance classifier to predict class probabilities\n",
    "abundance_prediction = svrClassifier.predict(X_test)\n",
    "# Concatenate predicted values to test data frame\n",
    "all_test = pd.concat([all_test, pd.DataFrame(abundance_prediction)], axis=1)\n",
    "all_test = all_test.rename(index=int, columns={0: 'svr_cover'})\n",
    "print('RF Cover trained...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = 'K:/VegetationEcology/Data_Harmonization/Project_GIS/Data_Output/testRegression/predicted.csv'\n",
    "all_test.to_csv(output_file, header=True, index=False, sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
