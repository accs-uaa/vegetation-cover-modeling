{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution-abundance Train and Test Random Forest All\n",
    "\n",
    "**Written by Timm Nawrocki**\n",
    "\n",
    "*Last updated Saturday, October 6, 2018.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# ---------------------------------------------------------------------------\n",
    "# Distribution-abundance Train and Test\n",
    "# Author: Timm Nawrocki, Alaska Center for Conservation Science\n",
    "# Created on: 2018-10-05\n",
    "# Usage: Must be executed as a Jupyter Notebook in an Anaconda 3 installation on a Google Cloud virtual machine with 64 vCPUs and 57.6 GB of CPU memory with an Ubuntu operating system (18.04 LTS).\n",
    "# Description: \"Distribution-Abundance Train and Test\" trains a multi-class classification model to determine species presence and absence and estimate species abundance. The probabilistic results are combined into a single continuous output that can theoretically range from 0 to 100 representing percent cover. All model performance metrics are calculated on independent test partitions.\n",
    "# ---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "This script runs the model train and test steps to output a model performance and variable importance report, trained classifier file, trained regressor file, and threshold files that can be transferred to the predict script. The script is formatted as a Jupyter Notebook and is intended to be run on a Google Cloud virtual machine with 64 vCPUs and 57.6 GB of CPU memory with an Ubuntu operating system (18.04 LTS). The classifier and regressor in this script are set to use 16 cores and may work inefficiently or not at all on a machine that has less than 64 cores. For information on generating inputs for this script or on setting up Google Cloud virtual machines, see the [project readme](https://github.com/accs-uaa/vegetation-cover-modeling).\n",
    "\n",
    "The distribution and abundance of a species is two distinct problems: 1. Where does the species occur? 2. Where the species occurs, how much is present? To represent the nested nature of the problems, we developed a hierarchical model where a classifier predicted species presence-absence and a regressor predicted species abundance in areas of predicted species presence. The practical advantage to this hierarchical method is that it accurately predicts zeros (i.e., absences), which are of disproportionate ecological value than exact prediction of other values along the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Data and Variables\n",
    "\n",
    "This script relies on data that has been pre-processed into a csv file using the ArcGIS Pro python tools available in the repository. The csv file must include all presence and absence observations for the target taxon or aggregate and all values for predictors extracted to the site locations. Variables defined below must be modified to match the input csv file if changes to the construction of features are made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input file\n",
    "input_file = '/home/twnawrocki/speciesData/carex_aquatilis.csv'\n",
    "# Define output folder\n",
    "output_folder = '/home/twnawrocki/modelResults/carex_aquatilis_rf_all'\n",
    "# Define output report\n",
    "output_report_name = 'carex-aquatilis-report.html'\n",
    "# Define species, genera, or aggregate name\n",
    "taxon_name = 'Carex aquatilis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variable sets\n",
    "predictor_metrics = ['compoundTopographic', 'dateFreeze_2000s', 'dateThaw_2000s', 'elevation', 'floodplainsDist', 'growingSeason_2000s', 'heatLoad', 'integratedMoisture', 'precipAnnual_2000s', 'roughness', 'siteExposure', 'slope', 'streamLargeDist', 'streamSmallDist', 'summerWarmth_2000s', 'surfaceArea', 'surfaceRelief', 'aspect', 'may_2_blue', 'may_evi2', 'may_nbr', 'may_ndmi', 'may_ndsi', 'may_ndvi', 'may_ndwi', 'june_2_blue', 'june_evi2', 'june_nbr', 'june_ndmi', 'june_ndsi', 'june_ndvi', 'june_ndwi', 'july_2_blue', 'july_evi2', 'july_nbr', 'july_ndmi', 'july_ndsi', 'july_ndvi', 'july_ndwi', 'august_2_blue', 'august_evi2', 'august_nbr', 'august_ndmi', 'august_ndsi', 'august_ndvi', 'august_ndwi', 'september_2_blue', 'september_evi2', 'september_nbr', 'september_ndmi', 'september_ndsi', 'september_ndvi', 'september_ndwi']\n",
    "zero_variable = ['zero']\n",
    "strata = ['strata']\n",
    "cover = ['cover']\n",
    "retain_variables = ['project', 'siteID', 'siteCode', 'methodSurvey', 'methodCover']\n",
    "coordinates = ['POINT_X', 'POINT_Y']\n",
    "all_variables = retain_variables + coordinates + predictor_metrics + zero_variable + strata + cover\n",
    "absence = ['absence']\n",
    "presence = ['presence']\n",
    "response = ['response']\n",
    "prediction = ['prediction']\n",
    "output_variables = all_variables + absence + presence + response + prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize\n",
    "\n",
    "This script has general dependencies on the *os* package for file system manipulations, the *numpy* and *pandas* packages for data manipulations, and the *seaborn* and *matplotlib* packages for plotting. *GPy* and *GPyOpt* are Gaussian Process packages that drive the bayesian optimization of hyperparameters. *XGBoost* provides the gradient boosting classifier and regressor used to create the composite predictions. *Scikit Learn* provides a random forest classifier and regressor for comparison to the gradient boosting performance, model selection and cross validation tools, performance metrics, and a function to save models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages for file manipulation, data manipulation, and plotting\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plot\n",
    "# Import module for altering output display\n",
    "from IPython.display import clear_output\n",
    "# Import packages for bayesian optimization\n",
    "import GPy\n",
    "import GPyOpt\n",
    "from GPyOpt.methods import BayesianOptimization\n",
    "# Import XGBoost gradient boosting implementations\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import XGBRegressor\n",
    "# Import modules for model selection, cross validation, random forest, and performance from Scikit Learn\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a plots folder if it does not exist\n",
    "plots_folder = os.path.join(output_folder, \"plots\")\n",
    "if not os.path.exists(plots_folder):\n",
    "    os.makedirs(plots_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output test data\n",
    "output_csv = os.path.join(output_folder, 'prediction.csv')\n",
    "# Define output model files\n",
    "output_classifier = os.path.join(output_folder, 'classifier.joblib')\n",
    "output_regressor = os.path.join(output_folder, 'regressor.joblib')\n",
    "# Define output threshold file\n",
    "threshold_file = os.path.join(output_folder, 'threshold.txt')\n",
    "# Define threshold and metrics table\n",
    "metrics_file = os.path.join(output_folder, 'metrics.csv')\n",
    "# Define output correlation plot\n",
    "variable_correlation = os.path.join(plots_folder, \"variable_correlation.png\")\n",
    "# Define output variable importance plots\n",
    "importance_classifier = os.path.join(plots_folder, \"importance_classifier.png\")\n",
    "importance_regressor = os.path.join(plots_folder, \"importance_regressor.png\")\n",
    "# Define output bayesian optimization convergence plots\n",
    "convergence_classifier = os.path.join(plots_folder, \"convergence_classifier.png\")\n",
    "convergence_regressor = os.path.join(plots_folder, \"convergence_regressor.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set initial plot sizefig_size = plot.rcParams[\"figure.figsize\"]\n",
    "fig_size = plot.rcParams[\"figure.figsize\"]\n",
    "fig_size[0] = 24\n",
    "fig_size[1] = 12\n",
    "plot.rcParams[\"figure.figsize\"] = fig_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Functions\n",
    "\n",
    "Analyses are conducted in units represented by functions. The functions are defined below in order of use. In general, functions in this script fall into three categories: Bayesian Optimization, Train and Test Iteration, and Export Results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Bayesian Optimization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an optimization objective function for the xgboost classifier\n",
    "def cvClassifier(parameters):\n",
    "    parameters = parameters[0]\n",
    "    score = cross_val_score(\n",
    "        XGBClassifier(max_depth=int(parameters[0]),\n",
    "                     learning_rate=parameters[1],\n",
    "                     n_estimators=int(parameters[2]),\n",
    "                     silent=True,\n",
    "                     objective='binary:logistic',\n",
    "                     booster='gbtree',\n",
    "                     n_jobs=1,\n",
    "                     gamma=parameters[3],\n",
    "                     min_child_weight=int(parameters[4]),\n",
    "                     max_delta_step=int(parameters[5]),\n",
    "                     subsample=parameters[6],\n",
    "                     colsample_bytree=parameters[7],\n",
    "                     colsample_bylevel=parameters[8],\n",
    "                     reg_alpha=parameters[9],\n",
    "                     reg_lambda=parameters[10],\n",
    "                     scale_pos_weight=parameters[11]),\n",
    "        X, y, scoring='roc_auc', cv=5).mean()\n",
    "    inverse_score = np.array((1-score)*-1)\n",
    "    return inverse_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an optimization objective function for the xgboost regressor\n",
    "def cvRegressor(parameters):\n",
    "    parameters = parameters[0]\n",
    "    score = cross_val_score(\n",
    "        XGBRegressor(max_depth=int(parameters[0]),\n",
    "                     learning_rate=parameters[1],\n",
    "                     n_estimators=int(parameters[2]),\n",
    "                     silent=True,\n",
    "                     objective='reg:linear',\n",
    "                     booster='gbtree',\n",
    "                     n_jobs=1,\n",
    "                     gamma=parameters[3],\n",
    "                     min_child_weight=int(parameters[4]),\n",
    "                     max_delta_step=int(parameters[5]),\n",
    "                     subsample=parameters[6],\n",
    "                     colsample_bytree=parameters[7],\n",
    "                     colsample_bylevel=parameters[8],\n",
    "                     reg_alpha=parameters[9],\n",
    "                     reg_lambda=parameters[10],\n",
    "                     scale_pos_weight=parameters[11]),\n",
    "        X, y, scoring='neg_mean_squared_error', cv=5).mean()\n",
    "    score = np.array(score)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an optimization function\n",
    "def bayesianOptimizer(objective_function, X, y, plot_file):\n",
    "    # Create the hyperparameter search domain\n",
    "    domain=[{'name': 'max_depth', 'type': 'discrete', 'domain': (5, 100)},\n",
    "            {'name': 'learning_rate', 'type': 'continuous', 'domain': (0, 1)},\n",
    "            {'name': 'n_estimators', 'type': 'discrete', 'domain': (50, 5000)},\n",
    "            {'name': 'gamma', 'type': 'continuous', 'domain': (0, 5)},\n",
    "            {'name': 'min_child_weight', 'type': 'discrete', 'domain': (0, 10)},\n",
    "            {'name': 'max_delta_step', 'type': 'continuous', 'domain': (0, 5)},\n",
    "            {'name': 'subsample', 'type': 'continuous', 'domain': (0.5, 1)},\n",
    "            {'name': 'colsample_bytree', 'type': 'continuous', 'domain': (0.3, 1.0)},\n",
    "            {'name': 'colsample_bylevel', 'type': 'continuous', 'domain': (0.3, 1.0)},\n",
    "            {'name': 'reg_alpha', 'type': 'continuous', 'domain': (1, 10)},\n",
    "            {'name': 'reg_lamda', 'type': 'continuous', 'domain': (1, 10)},\n",
    "            {'name': 'scale_pos_weight', 'type': 'continuous', 'domain': (1, 5)}]\n",
    "    # Initialize the Bayesian Optimizer\n",
    "    optimizer = BayesianOptimization(f=objective_function,\n",
    "                                     domain=domain,\n",
    "                                     model_type='GP',\n",
    "                                     initial_design_numdata=25,\n",
    "                                     initial_design_type='random',\n",
    "                                     acquisition_type ='EI',\n",
    "                                     exact_feval=False,\n",
    "                                     maximize=True)\n",
    "    # Run 1000 iterations of optimization\n",
    "    optimizer.run_optimization(max_iter=250)\n",
    "    # Plot the convergence of the best solution\n",
    "    optimizer.plot_convergence(filename=plot_file)\n",
    "    # Return results\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Train and Test Iteration Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to create the train and test partitions\n",
    "def partitionData(inData, all_variables, predictors, response, strata):\n",
    "    # Create train and test splits\n",
    "    X = inData[all_variables]\n",
    "    y = inData[response[0]]\n",
    "    stratify = inData[strata[0]]\n",
    "    all_train, all_test, y_train, y_test = train_test_split(X,\n",
    "                                                            y,\n",
    "                                                            test_size = 0.2,\n",
    "                                                            train_size = 0.8,\n",
    "                                                            random_state = None,\n",
    "                                                            shuffle = True,\n",
    "                                                            stratify = stratify)\n",
    "    # Reset the index on the train and test splits\n",
    "    all_train = all_train.reset_index()\n",
    "    all_test = all_test.reset_index()\n",
    "    return all_train, all_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to train a classifier on the train X and y and predict the test X\n",
    "def trainPredictClassifier(train, test, predictors, response):\n",
    "    # Define X and y\n",
    "    X_train = train[predictors]\n",
    "    y_train = train[response[0]]\n",
    "    X_test = test[predictors]\n",
    "    # Fit a classifier to the train X and y\n",
    "    classifier = RandomForestClassifier(n_estimators = 5000,\n",
    "                                        criterion = 'entropy',\n",
    "                                        max_features = 'log2',\n",
    "                                        bootstrap = True,\n",
    "                                        oob_score = False,\n",
    "                                        n_jobs = 1,\n",
    "                                        class_weight = 'balanced')\n",
    "    classifier.fit(X_train, y_train)\n",
    "    # Use the classifier to predict class probabilities\n",
    "    prediction = classifier.predict_proba(X_test)\n",
    "    # Concatenate predicted values to test data frame\n",
    "    test = pd.concat([test, pd.DataFrame(prediction)], axis=1)\n",
    "    test = test.rename(index=int, columns={0: 'absence', 1: 'presence'})\n",
    "    # Return the predictions\n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to train a regressor on the train X and y and predict the test X\n",
    "def trainPredictRegressor(train, test, predictors, response):\n",
    "    # Define X and y\n",
    "    X_train = train[predictors]\n",
    "    y_train = train[response[0]]\n",
    "    X_test = test[predictors]\n",
    "    # Fit a regressor to the train X and y\n",
    "    regressor = RandomForestRegressor(n_estimators=5000,\n",
    "                                      criterion='mse',\n",
    "                                      bootstrap=True,\n",
    "                                      oob_score=False,\n",
    "                                      n_jobs=1)\n",
    "    regressor.fit(X_train, y_train)\n",
    "    # Use the regressor to predict response values\n",
    "    prediction = regressor.predict(X_test)\n",
    "    # Concatenate predicted values to test data frame\n",
    "    test = pd.concat([test, pd.DataFrame(prediction)], axis=1)\n",
    "    test = test.rename(index=int, columns={0: 'response'})\n",
    "    # Return the predictions\n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate performance metrics based on a specified threshold value\n",
    "def testPresenceThreshold(predict_probability, threshold, y_test):\n",
    "    # Create an empty array of zeroes that matches the length of the probability predictions\n",
    "    predict_thresholded = np.zeros(predict_probability.shape)\n",
    "    # Set values for all probabilities greater than or equal to the threshold equal to 1\n",
    "    predict_thresholded[predict_probability >= threshold] = 1\n",
    "    # Determine error rates\n",
    "    confusion_test = confusion_matrix(y_test, predict_thresholded)\n",
    "    true_negative = confusion_test[0,0]\n",
    "    false_negative = confusion_test[1,0]\n",
    "    true_positive = confusion_test[1,1]\n",
    "    false_positive = confusion_test[0,1]\n",
    "    # Calculate sensitivity and specificity\n",
    "    sensitivity = true_positive / (true_positive + false_negative)\n",
    "    specificity = true_negative / (true_negative + false_positive)\n",
    "    # Calculate AUC score\n",
    "    auc = roc_auc_score(y_test, predict_probability)\n",
    "    # Calculate overall accuracy\n",
    "    accuracy = (true_negative + true_positive) / (true_negative + false_positive + false_negative + true_positive)\n",
    "    # Return the thresholded probabilities and the performance metrics\n",
    "    return (sensitivity, specificity, auc, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to determine a presence threshold\n",
    "def determineOptimalThreshold(predict_probability, y_test):\n",
    "    # Iterate through numbers between 0 and 1000 to output a list of sensitivity and specificity values per threshold number\n",
    "    i = 1\n",
    "    sensitivity_list = []\n",
    "    specificity_list = []\n",
    "    while i < 1001:\n",
    "        threshold = i/1000\n",
    "        sensitivity, specificity, auc, accuracy = testPresenceThreshold(predict_probability, threshold, y_test)\n",
    "        sensitivity_list.append(sensitivity)\n",
    "        specificity_list.append(specificity)\n",
    "        i = i + 1\n",
    "    # Calculate a list of absolute value difference between sensitivity and specificity and find the optimal threshold\n",
    "    difference_list = [np.absolute(a - b) for a, b in zip(sensitivity_list, specificity_list)]\n",
    "    value, threshold = min((value, threshold) for (threshold, value) in enumerate(difference_list))\n",
    "    threshold = threshold/1000\n",
    "    # Calculate the performance of the optimal threshold\n",
    "    sensitivity, specificity, auc, accuracy = testPresenceThreshold(predict_probability, threshold, y_test)\n",
    "    # Return the optimal threshold and the performance metrics of the optimal threshold\n",
    "    return threshold, sensitivity, specificity, auc, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to composite model results\n",
    "def compositePrediction(test, presence, response, threshold):\n",
    "    # Define a function to threshold absences and set presences equal to regression response\n",
    "    def compositeRows(row):\n",
    "        if row[presence[0]] < threshold:\n",
    "            return 0\n",
    "        elif row[presence[0]] >= threshold:\n",
    "            return row[response[0]]\n",
    "    # Apply function to all rows in test data\n",
    "    test['prediction'] = test.apply(lambda row: compositeRows(row), axis=1)\n",
    "    # Return the test data frame with composited results\n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to calculate pseudo r-squared and RMSE for the composited prediction\n",
    "def calculatePerformance(test, cover, prediction):\n",
    "    # Define the true values and the predicted values for the response variable\n",
    "    y_test = test[cover[0]]\n",
    "    y_prediction = test[prediction[0]]\n",
    "    # Calculate pseudo r-squared\n",
    "    r_score = r2_score(y_test, y_prediction, sample_weight=None, multioutput='uniform_average')\n",
    "    # Calculate error\n",
    "    mae = mean_absolute_error(y_test, y_prediction)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_prediction))\n",
    "    # Return performance metrics\n",
    "    return r_score, mae, rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Export Results Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot Pearson correlation of predictor variables\n",
    "def plotVariableCorrelation(X_train, outFile):\n",
    "    # Calculate Pearson correlation coefficient between the predictor variables, where -1 is perfect negative correlation and 1 is perfect positive correlation\n",
    "    correlation = X_train.astype('float64').corr()\n",
    "    # Generate a mask for the upper triangle of plot\n",
    "    mask = np.zeros_like(correlation, dtype=np.bool)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "    # Set up the matplotlib figure\n",
    "    f, ax = plot.subplots(figsize=(11, 9))\n",
    "    # Generate a custom diverging colormap\n",
    "    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "    # Draw the heatmap with the mask and correct aspect ratio\n",
    "    correlation_plot = sns.heatmap(correlation, mask=mask, cmap=cmap, vmax=.3, center=0, square=True, linewidths=.5, cbar_kws={'shrink': .5})\n",
    "    correlation_figure = correlation_plot.get_figure()\n",
    "    correlation_figure.savefig(outFile, bbox_inches='tight', dpi=300)\n",
    "    # Clear plot workspace\n",
    "    plot.clf()\n",
    "    plot.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot variable importances\n",
    "def plotVariableImportances(inModel, X_train, outVariableFile):\n",
    "    # Get numerical feature importances\n",
    "    importances = list(inModel.feature_importances_)\n",
    "    # List of tuples with variable and importance\n",
    "    feature_list = list(X_train.columns)\n",
    "    feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "    # Sort the feature importances by most important first\n",
    "    feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "    # Initialize the plot and set figure size\n",
    "    variable_figure = plot.figure()\n",
    "    plot.style.use('fivethirtyeight')\n",
    "    fig_size = plot.rcParams[\"figure.figsize\"]\n",
    "    fig_size[0] = 36\n",
    "    fig_size[1] = 12\n",
    "    plot.rcParams[\"figure.figsize\"] = fig_size\n",
    "    # Create list of x locations for plotting\n",
    "    x_values = list(range(len(importances)))\n",
    "    # Make a bar chart of the variable importances\n",
    "    plot.bar(x_values, importances, orientation = 'vertical')\n",
    "    # Tick labels for x axis\n",
    "    plot.xticks(x_values, feature_list, rotation='vertical')\n",
    "    # Axis labels and title\n",
    "    plot.ylabel('Importance'); plot.xlabel('Variable'); plot.title('Variable Importances');\n",
    "    # Export\n",
    "    variable_figure.savefig(outVariableFile, bbox_inches=\"tight\", dpi=300)\n",
    "    # Clear plot workspace\n",
    "    plot.clf()\n",
    "    plot.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to train and export a classifier\n",
    "def trainExportClassifier(input_data, predictors, response, outModel, outImportance):\n",
    "    # Define the predictor labels (X) and the response label (y) in the input dataframe\n",
    "    X = input_data[predictors]\n",
    "    y = input_data[response[0]]\n",
    "    # Fit a classifier to the input dataset\n",
    "    classifier = RandomForestClassifier(n_estimators = 5000,\n",
    "                                        criterion = 'entropy',\n",
    "                                        max_features = 'log2',\n",
    "                                        bootstrap = True,\n",
    "                                        oob_score = False,\n",
    "                                        n_jobs = 1,\n",
    "                                        class_weight = 'balanced')\n",
    "    classifier.fit(X, y)\n",
    "    # Save classifier to an external file\n",
    "    joblib.dump(classifier, outModel)\n",
    "    # Export a variable importance plot\n",
    "    plotVariableImportances(classifier, X, outImportance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainExportRegressor(input_data, predictors, response, outModel, outImportance):\n",
    "    # Define the predictor labels (X) and the response label (y) in the input dataframe\n",
    "    X = input_data[predictors]\n",
    "    y = input_data[response[0]]\n",
    "    # Fit a classifier to the input dataset\n",
    "    regressor = RandomForestRegressor(n_estimators=5000,\n",
    "                                      criterion='mse',\n",
    "                                      bootstrap=True,\n",
    "                                      oob_score=False,\n",
    "                                      n_jobs=1)\n",
    "    regressor.fit(X, y)\n",
    "    # Save classifier to an external file\n",
    "    joblib.dump(regressor, outModel)\n",
    "    # Export a variable importance plot\n",
    "    plotVariableImportances(regressor, X, outImportance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conduct Analyses\n",
    "\n",
    "The analyses are subdivided into subsections: load data, bayesian optimization, train and test iterations, and export results. The majority of analytical time is devoted to conducting bayesian optimization iterations and train and test iterations. The XGBoost implementation of gradient boosting is the primary algorithm. Random forest is included as a baseline to exceed with tuned gradient boosting.\n",
    "\n",
    "We use a Bayesian statistical framework with Gaussian Process as the generative model to find an optimal set of hyperparameters for the classifier and regressor. The Bayesian Optimization tunes species-specific models that make the best generalizations as estimated by 5-fold cross validation. The domain space for bayesian optimization is large, so we initially sample 50 points at random and then conduct 1000 optimization iterations to converge on a best set of hyperparameters. Optimization occurs independently for the classifier and the regressor. The performance of each optimization run is determined using 5-fold cross validation.\n",
    "\n",
    "The optimal hyperparameters are passed into 100 iterations of training and testing in which the results of the classifier and regressor are composited to determine overall performance. For each iteration, we withhold 20% of the data at random stratified by broad cover ranges as an independent test partition. The composited prediction results are appended into a single data frame for additional analyses and plotting external to this script.\n",
    "\n",
    "Outputs of the analysis are:\n",
    "1. Report of performance, including the following plots:\n",
    "  1. Convergence of the classifier hyperparameters\n",
    "  2. Convergence of the regressor hyperparameters\n",
    "  3. Variable importances of the classifier\n",
    "  4. Variable importances of the regressor\n",
    "  5. Pearson correlation for all predictors\n",
    "2. Model files:\n",
    "  1. Classifier\n",
    "  2. Regressor\n",
    "3. Table of performance metrics\n",
    "4. Mean threshold\n",
    "5. Test predictions from 100 validation iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data frame of input data\n",
    "input_data = pd.read_csv(input_file)\n",
    "# Convert values to floats\n",
    "input_data[predictor_metrics + cover + coordinates] = input_data[predictor_metrics + cover + coordinates].astype(float)\n",
    "# Convert values to integers\n",
    "input_data[strata + zero_variable] = input_data[strata + zero_variable].astype(int)\n",
    "# Shuffle data\n",
    "input_data = shuffle(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set initial plot sizefig_size = plot.rcParams[\"figure.figsize\"]\n",
    "fig_size = plot.rcParams[\"figure.figsize\"]\n",
    "fig_size[0] = 36\n",
    "fig_size[1] = 12\n",
    "plot.rcParams[\"figure.figsize\"] = fig_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Bayesian Optimization\n",
    "\n",
    "The Gradient Boosting Classifier was optimized using all available data, and the performance was evaluated as area under the receiver operating characteristic curve (AUC). For comparison, the performance of a Random Forest Classifier was also tested using 5-fold cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gradient Boosting Regressor was optimized using presence data only, and the performance was evaluated as negative mean squared error (-MSE). For ease of interpretation, we report root mean squared error (RMSE) rather than -MSE. For comparison, the performance of a Random Forest Regressor was also tested using 5-fold cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Train and Test Iterations (n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty lists to store threshold and performance metrics\n",
    "threshold_list = []\n",
    "sensitivity_list = []\n",
    "specificity_list = []\n",
    "auc_list = []\n",
    "accuracy_list = []\n",
    "r_score_list = []\n",
    "mae_list = []\n",
    "rmse_list = []\n",
    "# Create an empty data frame to store the test results\n",
    "output_test = pd.DataFrame(columns=output_variables)\n",
    "i = 1\n",
    "while i < 101:\n",
    "    # Set output display to show one message with replacement\n",
    "    clear_output(wait=True)\n",
    "    # Split the data into train and test partitions\n",
    "    all_train, all_test = partitionData(input_data, all_variables, predictor_metrics, zero_variable, strata)\n",
    "    # Train and predict a classifier\n",
    "    all_test = trainPredictClassifier(all_train, all_test, predictor_metrics, zero_variable)\n",
    "    # Subset the training data to quantitative presences\n",
    "    presence_train = all_train[all_train[cover[0]] >= 1]\n",
    "    # Train and predict a regressor\n",
    "    all_test = trainPredictRegressor(presence_train, all_test, predictor_metrics, cover)\n",
    "    # Calculate the optimal threshold and performance of the presence-absence classification\n",
    "    threshold, sensitivity, specificity, auc, accuracy = determineOptimalThreshold(all_test[presence[0]], all_test[zero_variable[0]])\n",
    "    # Composite the classifier and regressor predictions\n",
    "    all_test = compositePrediction(all_test, presence, response, threshold)\n",
    "    # Subset the test data to quantitative\n",
    "    quantitative_test = all_test\n",
    "    # Calculate overall performance\n",
    "    r_score, mae, rmse = calculatePerformance(quantitative_test, cover, prediction)\n",
    "    # Add threshold and performance metrics to list\n",
    "    threshold_list.append(threshold)\n",
    "    sensitivity_list.append(sensitivity)\n",
    "    specificity_list.append(specificity)\n",
    "    auc_list.append(auc)\n",
    "    accuracy_list.append(accuracy)\n",
    "    r_score_list.append(r_score)\n",
    "    mae_list.append(mae)\n",
    "    rmse_list.append(rmse)\n",
    "    # Add the test results to output data frame\n",
    "    output_test = output_test.append(all_test, ignore_index=True, sort=True)\n",
    "    print('Model train-test iteration ' + str(i) + ' out of 100 completed...')\n",
    "    # Increase the counter by 1\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export test results to csv\n",
    "output_test.to_csv(output_csv, header=True, index=False, sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set initial plot sizefig_size = plot.rcParams[\"figure.figsize\"]\n",
    "fig_size = plot.rcParams[\"figure.figsize\"]\n",
    "fig_size[0] = 16\n",
    "fig_size[1] = 12\n",
    "plot.rcParams[\"figure.figsize\"] = fig_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export a Pearson Correlation plot for the predictor variables\n",
    "plotVariableCorrelation(input_data[predictor_metrics], variable_correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set initial plot sizefig_size = plot.rcParams[\"figure.figsize\"]\n",
    "fig_size = plot.rcParams[\"figure.figsize\"]\n",
    "fig_size[0] = 36\n",
    "fig_size[1] = 12\n",
    "plot.rcParams[\"figure.figsize\"] = fig_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and export a final classifier using the full input data\n",
    "trainExportClassifier(input_data,\n",
    "                      predictor_metrics,\n",
    "                      zero_variable,\n",
    "                      output_classifier,\n",
    "                      importance_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset the input data to presences only\n",
    "presence_data = input_data[input_data[cover[0]] >= 1]\n",
    "# Train and export a final regressor using the full input data\n",
    "trainExportRegressor(presence_data,\n",
    "                     predictor_metrics,\n",
    "                     cover,\n",
    "                     output_regressor,\n",
    "                     importance_regressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean for threshold and all performance metrics\n",
    "threshold_mean = np.mean(threshold_list)\n",
    "sensitivity_mean = np.mean(sensitivity_list)\n",
    "specificity_mean = np.mean(specificity_list)\n",
    "auc_mean = np.mean(auc_list)\n",
    "accuracy_mean = np.mean(accuracy_list)\n",
    "r_score_mean = np.mean(r_score_list)\n",
    "mae_mean = np.mean(mae_list)\n",
    "rmse_mean = np.mean(rmse_list)\n",
    "# Calculate standard deviation for threshold and all performance metrics\n",
    "threshold_sd = np.std(threshold_list)\n",
    "sensitivity_sd = np.std(sensitivity_list)\n",
    "specificity_sd = np.std(specificity_list)\n",
    "auc_sd = np.std(auc_list)\n",
    "accuracy_sd = np.std(accuracy_list)\n",
    "r_score_sd = np.std(r_score_list)\n",
    "mae_sd = np.std(mae_list)\n",
    "rmse_sd = np.std(rmse_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export threshold and performance metrics as a table\n",
    "metrics_dataframe = pd.DataFrame({'threshold':threshold_list,\n",
    "                                  'sensitivity':sensitivity_list,\n",
    "                                  'specificity':specificity_list,\n",
    "                                  'auc':auc_list,\n",
    "                                  'accuracy':accuracy_list,\n",
    "                                  'r_score':r_score_list,\n",
    "                                  'mae':mae_list,\n",
    "                                  'rmse':rmse_list})\n",
    "metrics_dataframe.to_csv(metrics_file, header=True, index=False, sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a text file to store the presence-absence conversion threshold\n",
    "file = open(threshold_file, 'w')\n",
    "file.write(str(round(threshold_mean, 5)))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write html text file\n",
    "output_report = os.path.join(output_folder, output_report_name)\n",
    "output_text = os.path.splitext(output_report)[0] + \".txt\"\n",
    "text_file = open(output_text, \"w\")\n",
    "text_file.write(\"<html>\\n\")\n",
    "text_file.write(\"<head>\\n\")\n",
    "text_file.write(\"<meta http-equiv=\\\"pragma\\\" content=\\\"no-cache\\\">\\n\")\n",
    "text_file.write(\"<meta http-equiv=\\\"Expires\\\" content=\\\"-1\\\">\\n\")\n",
    "text_file.write(\"</head>\\n\")\n",
    "text_file.write(\"<body>\\n\")\n",
    "text_file.write(\"<div style=\\\"width:90%;max-width:1000px;margin-left:auto;margin-right:auto\\\">\\n\")\n",
    "text_file.write(\"<h1 style=\\\"text-align:center;\\\">Distribution-abundance model performance for \" + taxon_name + \"</h1>\\n\")\n",
    "text_file.write(\"<br>\" + \"\\n\")\n",
    "text_file.write(\"<h2>Predicted Distribution-abundance Pattern</h2>\\n\")\n",
    "text_file.write(\"<p>Distribution-abundance was predicted by a composite hierarchical model: 1. a classifier predicted species presence or absence, and 2. a regressor predicted species proportional abundance in areas where the classifier predicted the species is present. The map below shows the output raster prediction.</p>\\n\")\n",
    "text_file.write(\"<p><i>Prediction step has not yet been performed. No output to display.</i></p>\\n\")\n",
    "text_file.write(\"<h2>Composite Model Performance</h2>\\n\")\n",
    "text_file.write(\"<p>Model performance was measured by pseudo r squared, mean absolute error, and root mean squared error for the composite prediction. Additionally, the performance of the absence class is reported as an area under the receiver operating characteristic curve (AUC) and overall accuracy, where specificity and sensitivity are as close to equal as possible (i.e, the model performs equally well at predicting absences and presences). All performance results are reported as the mean and standard deviation of the independent test predictions from 100 train-test splits. For each split, 30% of the data was withheld from model training and used as an independent test partition. The test partition was randomly selected from cover class strata: 0%, 1-10%, 11-25%, and 26-100%.</p>\\n\")\n",
    "text_file.write(\"<h3>Overall Performance</h3>\\n\")\n",
    "text_file.write(\"<p>Pseudo r-squared = \" + str(np.round(r_score_mean, 2)) + \" +/- \" + str(np.round(r_score_sd, 2)) +\"</p>\\n\")\n",
    "text_file.write(\"<p>Mean Absolute Error = \" + str(np.round(mae_mean, 2)) + \" +/- \" + str(np.round(mae_sd, 2)) +\"</p>\\n\")\n",
    "text_file.write(\"<p>Root Mean Squared Error = \" + str(np.round(rmse_mean, 2)) + \" +/- \" + str(np.round(rmse_sd, 2)) +\"</p>\\n\")\n",
    "text_file.write(\"<h3>Absence Performance</h3>\\n\")\n",
    "text_file.write(\"<p>AUC = \" + str(np.round(auc_mean, 2)) + \" +/- \" + str(np.round(auc_sd, 2)) +\"</p>\\n\")\n",
    "text_file.write(\"<p>Presence-Absence Accuracy = \" + str(np.round(accuracy_mean, 2)) + \" +/- \" + str(np.round(accuracy_sd, 2)) +\"</p>\\n\")\n",
    "text_file.write(\"<h3>Classifier Importances</h3>\\n\")\n",
    "text_file.write(\"<p>The Variable Importance plot for the classifier is shown below:</p>\\n\")\n",
    "text_file.write(\"<a target='_blank' href='plots\\\\importance_classifier.png'><img style='display:inline-block;max-width:1000px;width:100%;' src='plots\\\\importance_classifier.png'></a>\\n\")\n",
    "text_file.write(\"<h3>Regressor Importances</h3>\\n\")\n",
    "text_file.write(\"<p>The Variable Importance plot for the regressor is shown below:</p>\\n\")\n",
    "text_file.write(\"<a target='_blank' href='plots\\\\importance_regressor.png'><img style='display:inline-block;max-width:1000px;width:100%;' src='plots\\\\importance_regressor.png'></a>\\n\")\n",
    "text_file.write(r\"<h2>Variable Correlation</h2>\" + \"\\n\")\n",
    "text_file.write(\"<p>The plot below explores variable correlation. No attempt was made to remove highly correlated variables (shown in the plot dark blue).</p>\\n\")\n",
    "text_file.write(\"<a target='_blank' href='plots\\\\variable_correlation.png'><img style='display:inline-block;width:100%;' src='plots\\\\variable_correlation.png'></a>\\n\")\n",
    "text_file.write(\"</div>\\n\")\n",
    "text_file.write(\"</body>\\n\")\n",
    "text_file.write(\"</html>\\n\")\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename HTML Text to HTML\n",
    "if os.path.exists(output_report) == True:\n",
    "    os.remove(output_report)\n",
    "os.rename(output_text, output_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
