{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All modules successfully imported.\n"
     ]
    }
   ],
   "source": [
    "# Import modules\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plot\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.linear_model import ARDRegression, BayesianRidge\n",
    "from collections import Counter\n",
    "print('All modules successfully imported.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable sets loaded.\n"
     ]
    }
   ],
   "source": [
    "# Define variable sets\n",
    "predictor_metrics = ['compoundTopographic', 'dateFreeze_2000s', 'dateThaw_2000s', 'elevation', 'floodplainsDist', 'growingSeason_2000s', 'heatLoad', 'integratedMoisture', 'precipAnnual_2000s', 'roughness', 'siteExposure', 'slope', 'streamLargeDist', 'streamSmallDist', 'summerWarmth_2000s', 'surfaceArea', 'surfaceRelief', 'aspect', 'may_2_blue', 'may_evi2', 'may_nbr', 'may_ndmi', 'may_ndsi', 'may_ndvi', 'may_ndwi', 'june_2_blue', 'june_evi2', 'june_nbr', 'june_ndmi', 'june_ndsi', 'june_ndvi', 'june_ndwi', 'july_2_blue', 'july_evi2', 'july_nbr', 'july_ndmi', 'july_ndsi', 'july_ndvi', 'july_ndwi', 'august_2_blue', 'august_evi2', 'august_nbr', 'august_ndmi', 'august_ndsi', 'august_ndvi', 'august_ndwi', 'september_2_blue', 'september_evi2', 'september_nbr', 'september_ndmi', 'september_ndsi', 'september_ndvi', 'september_ndwi']\n",
    "zero_variable = ['zero']\n",
    "cover = ['cover']\n",
    "coverLog = ['coverLog']\n",
    "strata = ['strata']\n",
    "retain_variables = ['project', 'siteID', 'siteCode', 'methodSurvey', 'methodCover']\n",
    "coordinates = ['POINT_X', 'POINT_Y']\n",
    "all_variables = retain_variables + coordinates + predictor_metrics + zero_variable + strata + cover + coverLog\n",
    "scale_variables = predictor_metrics\n",
    "print('Variable sets loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define raw train and test data\n",
    "train_file = 'E:/VegetationEcology/Data_Harmonization/Project_GIS/Data_Output/testRegression/train_scaled.csv'\n",
    "test_file = 'E:/VegetationEcology/Data_Harmonization/Project_GIS/Data_Output/testRegression/test_scaled.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data frames of train and test data\n",
    "all_train = pd.read_csv(train_file)\n",
    "all_test = pd.read_csv(test_file)\n",
    "\n",
    "# Convert values to floats\n",
    "all_train[predictor_metrics + cover + coverLog + coordinates] = all_train[predictor_metrics + cover + coverLog + coordinates].astype(float)\n",
    "all_test[predictor_metrics + cover + coverLog + coordinates] = all_test[predictor_metrics + cover + coverLog + coordinates].astype(float)\n",
    "\n",
    "# Convert values to integers\n",
    "all_train[strata + zero_variable] = all_train[strata + zero_variable].astype(int)\n",
    "all_test[strata + zero_variable] = all_test[strata + zero_variable].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the response variable to a gaussian distribution\n",
    "y_scaler = StandardScaler()\n",
    "y_scaler.fit(all_train[coverLog])\n",
    "# Transform the training data\n",
    "train_scaled = y_scaler.transform(all_train[coverLog])\n",
    "all_train.drop(labels=coverLog, axis='columns', inplace=True)\n",
    "all_train = pd.concat([all_train, pd.DataFrame(data=train_scaled, columns=coverLog)], axis=1)\n",
    "# Transform the test data\n",
    "test_scaled = y_scaler.transform(all_test[coverLog])\n",
    "all_test.drop(labels=coverLog, axis='columns', inplace=True)\n",
    "all_test = pd.concat([all_test, pd.DataFrame(data=test_scaled, columns=coverLog)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create stratified training partitions\n",
    "def stratifyTrainingData(inData):\n",
    "    # Break training data into individual strata\n",
    "    train_1 = inData[inData[strata[0]] == 1]\n",
    "    train_2 = inData[inData[strata[0]] == 2]\n",
    "    train_3 = inData[inData[strata[0]] == 3]\n",
    "    # Determine sample size for each strata\n",
    "    n_1 = len(train_1[cover])\n",
    "    n_2 = len(train_2[cover])\n",
    "    n_3 = len(train_3[cover])\n",
    "    # Determine the minimum sample size for the training dataset\n",
    "    n_min = int(min([n_1, n_2, n_3]))\n",
    "    print('Minimum sample size is ' + str(n_min))\n",
    "    # Determine sampling ratio for each class\n",
    "    p_1 = round(n_min/n_1, 2)\n",
    "    p_2 = round(n_min/n_2, 2)\n",
    "    p_3 = round(n_min/n_3, 2)\n",
    "    return train_1, train_2, train_3, p_1, p_2, p_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum sample size is 80\n"
     ]
    }
   ],
   "source": [
    "# Break training data into individual strata\n",
    "train_10, train_25, train_100, p_10, p_25, p_100 = stratifyTrainingData(all_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create a training sample\n",
    "def createTrainingSample(strata_1, strata_2, strata_3, p_1, p_2, p_3):\n",
    "    # Draw a sample from strata 1\n",
    "    if p_1 < 1:\n",
    "        X = strata_1[all_variables]\n",
    "        y = strata_1[cover[0]]\n",
    "        train_1, test_1, y_train_1, y_test_1 = train_test_split(X, y, test_size = 1-p_1, train_size = p_1, random_state = None, shuffle = True, stratify = None)\n",
    "    elif p_1 == 1:\n",
    "        train_1 = strata_1\n",
    "    # Draw a sample from strata 2\n",
    "    if p_2 < 1:\n",
    "        X = strata_2[all_variables]\n",
    "        y = strata_2[cover[0]]\n",
    "        train_2, test_2, y_train_2, y_test_2 = train_test_split(X, y, test_size = 1-p_2, train_size = p_2, random_state = None, shuffle = True, stratify = None)\n",
    "    elif p_2 == 1:\n",
    "        train_2 = strata_2\n",
    "    # Draw a sample from strata 3\n",
    "    if p_3 < 1:\n",
    "        X = strata_3[all_variables]\n",
    "        y = strata_3[cover[0]]\n",
    "        train_3, test_3, y_train_3, y_test_3 = train_test_split(X, y, test_size = 1-p_3, train_size = p_3, random_state = None, shuffle = True, stratify = None)\n",
    "    elif p_3 == 1:\n",
    "        train_3 = strata_3\n",
    "    # Build training sample\n",
    "    train_sample = train_1.append(train_2, ignore_index = True, sort = True)\n",
    "    train_sample = train_sample.append(train_3, ignore_index = True, sort = True)\n",
    "    train_sample.reset_index()\n",
    "    return train_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create 10 trees\n",
    "def trainRegressor(train_sample, predictors, response):\n",
    "    # Define the X and y values\n",
    "    X_train = train_sample[predictors]\n",
    "    y_train = train_sample[response[0]]\n",
    "    # Fit a regressor to the training dataset\n",
    "    regressor = RandomForestRegressor(n_estimators=10, criterion='mse', bootstrap=True, oob_score=False, n_jobs=1)\n",
    "    regressor.fit(X_train, y_train)\n",
    "    return regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to combine random forest estimators\n",
    "def combineRegressors(meta_regressor, regressor):\n",
    "    meta_regressor.estimators_ += regressor.estimators_\n",
    "    meta_regressor.n_estimators = len(meta_regressor.estimators_)\n",
    "    return meta_regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to train a meta regressor\n",
    "def trainMetaRegressor(strata_1, strata_2, strata_3, p_1, p_2, p_3, predictors, response, test_data):\n",
    "    # Set counter\n",
    "    i = 1\n",
    "    # Create initial training sample\n",
    "    initial_train = createTrainingSample(strata_1, strata_2, strata_3, p_1, p_2, p_3)\n",
    "    # Train an initial meta-regressor\n",
    "    meta_regressor = trainRegressor(initial_train, predictors, response)\n",
    "    # Increase the counter\n",
    "    print('Model iteration ' + str(i) + ' out of 100 trained and tested...')\n",
    "    i = i + 1\n",
    "    \n",
    "    # Conduct 99 additional regressor training iterations and merge trees into meta regressor\n",
    "    while i < 101:\n",
    "        # Create a training sample\n",
    "        train_sample = createTrainingSample(strata_1, strata_2, strata_3, p_1, p_2, p_3)\n",
    "        # Train a regressor\n",
    "        regressor = trainRegressor(train_sample, predictors, response)\n",
    "        # Merge trees into the meta regressor\n",
    "        meta_regressor = combineRegressors(meta_regressor, regressor)\n",
    "        # Increase the counter\n",
    "        print('Model iteration ' + str(i) + ' out of 100 trained and tested...')\n",
    "        i = i + 1\n",
    "    \n",
    "    # Prepare the test data\n",
    "    X_test = test_data[predictors]\n",
    "    y_test = test_data[response[0]]\n",
    "    # Use the meta regressor to predict values for the test dataset\n",
    "    test_prediction = meta_regressor.predict(X_test)\n",
    "    # Concatenate predicted values to test data frame\n",
    "    test_data = pd.concat([test_data, pd.DataFrame(test_prediction)], axis=1)\n",
    "    test_data = test_data.rename(index=int, columns={0: 'regress'})\n",
    "    return meta_regressor, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model iteration 1 out of 100 trained and tested...\n",
      "Model iteration 2 out of 100 trained and tested...\n",
      "Model iteration 3 out of 100 trained and tested...\n",
      "Model iteration 4 out of 100 trained and tested...\n",
      "Model iteration 5 out of 100 trained and tested...\n",
      "Model iteration 6 out of 100 trained and tested...\n",
      "Model iteration 7 out of 100 trained and tested...\n",
      "Model iteration 8 out of 100 trained and tested...\n",
      "Model iteration 9 out of 100 trained and tested...\n",
      "Model iteration 10 out of 100 trained and tested...\n",
      "Model iteration 11 out of 100 trained and tested...\n",
      "Model iteration 12 out of 100 trained and tested...\n",
      "Model iteration 13 out of 100 trained and tested...\n",
      "Model iteration 14 out of 100 trained and tested...\n",
      "Model iteration 15 out of 100 trained and tested...\n",
      "Model iteration 16 out of 100 trained and tested...\n",
      "Model iteration 17 out of 100 trained and tested...\n",
      "Model iteration 18 out of 100 trained and tested...\n",
      "Model iteration 19 out of 100 trained and tested...\n",
      "Model iteration 20 out of 100 trained and tested...\n",
      "Model iteration 21 out of 100 trained and tested...\n",
      "Model iteration 22 out of 100 trained and tested...\n",
      "Model iteration 23 out of 100 trained and tested...\n",
      "Model iteration 24 out of 100 trained and tested...\n",
      "Model iteration 25 out of 100 trained and tested...\n",
      "Model iteration 26 out of 100 trained and tested...\n",
      "Model iteration 27 out of 100 trained and tested...\n",
      "Model iteration 28 out of 100 trained and tested...\n",
      "Model iteration 29 out of 100 trained and tested...\n",
      "Model iteration 30 out of 100 trained and tested...\n",
      "Model iteration 31 out of 100 trained and tested...\n",
      "Model iteration 32 out of 100 trained and tested...\n",
      "Model iteration 33 out of 100 trained and tested...\n",
      "Model iteration 34 out of 100 trained and tested...\n",
      "Model iteration 35 out of 100 trained and tested...\n",
      "Model iteration 36 out of 100 trained and tested...\n",
      "Model iteration 37 out of 100 trained and tested...\n",
      "Model iteration 38 out of 100 trained and tested...\n",
      "Model iteration 39 out of 100 trained and tested...\n",
      "Model iteration 40 out of 100 trained and tested...\n",
      "Model iteration 41 out of 100 trained and tested...\n",
      "Model iteration 42 out of 100 trained and tested...\n",
      "Model iteration 43 out of 100 trained and tested...\n",
      "Model iteration 44 out of 100 trained and tested...\n",
      "Model iteration 45 out of 100 trained and tested...\n",
      "Model iteration 46 out of 100 trained and tested...\n",
      "Model iteration 47 out of 100 trained and tested...\n",
      "Model iteration 48 out of 100 trained and tested...\n",
      "Model iteration 49 out of 100 trained and tested...\n",
      "Model iteration 50 out of 100 trained and tested...\n",
      "Model iteration 51 out of 100 trained and tested...\n",
      "Model iteration 52 out of 100 trained and tested...\n",
      "Model iteration 53 out of 100 trained and tested...\n",
      "Model iteration 54 out of 100 trained and tested...\n",
      "Model iteration 55 out of 100 trained and tested...\n",
      "Model iteration 56 out of 100 trained and tested...\n",
      "Model iteration 57 out of 100 trained and tested...\n",
      "Model iteration 58 out of 100 trained and tested...\n",
      "Model iteration 59 out of 100 trained and tested...\n",
      "Model iteration 60 out of 100 trained and tested...\n",
      "Model iteration 61 out of 100 trained and tested...\n",
      "Model iteration 62 out of 100 trained and tested...\n",
      "Model iteration 63 out of 100 trained and tested...\n",
      "Model iteration 64 out of 100 trained and tested...\n",
      "Model iteration 65 out of 100 trained and tested...\n",
      "Model iteration 66 out of 100 trained and tested...\n",
      "Model iteration 67 out of 100 trained and tested...\n",
      "Model iteration 68 out of 100 trained and tested...\n",
      "Model iteration 69 out of 100 trained and tested...\n",
      "Model iteration 70 out of 100 trained and tested...\n",
      "Model iteration 71 out of 100 trained and tested...\n",
      "Model iteration 72 out of 100 trained and tested...\n",
      "Model iteration 73 out of 100 trained and tested...\n",
      "Model iteration 74 out of 100 trained and tested...\n",
      "Model iteration 75 out of 100 trained and tested...\n",
      "Model iteration 76 out of 100 trained and tested...\n",
      "Model iteration 77 out of 100 trained and tested...\n",
      "Model iteration 78 out of 100 trained and tested...\n",
      "Model iteration 79 out of 100 trained and tested...\n",
      "Model iteration 80 out of 100 trained and tested...\n",
      "Model iteration 81 out of 100 trained and tested...\n",
      "Model iteration 82 out of 100 trained and tested...\n",
      "Model iteration 83 out of 100 trained and tested...\n",
      "Model iteration 84 out of 100 trained and tested...\n",
      "Model iteration 85 out of 100 trained and tested...\n",
      "Model iteration 86 out of 100 trained and tested...\n",
      "Model iteration 87 out of 100 trained and tested...\n",
      "Model iteration 88 out of 100 trained and tested...\n",
      "Model iteration 89 out of 100 trained and tested...\n",
      "Model iteration 90 out of 100 trained and tested...\n",
      "Model iteration 91 out of 100 trained and tested...\n",
      "Model iteration 92 out of 100 trained and tested...\n",
      "Model iteration 93 out of 100 trained and tested...\n",
      "Model iteration 94 out of 100 trained and tested...\n",
      "Model iteration 95 out of 100 trained and tested...\n",
      "Model iteration 96 out of 100 trained and tested...\n",
      "Model iteration 97 out of 100 trained and tested...\n",
      "Model iteration 98 out of 100 trained and tested...\n",
      "Model iteration 99 out of 100 trained and tested...\n",
      "Model iteration 100 out of 100 trained and tested...\n"
     ]
    }
   ],
   "source": [
    "# Train the meta regressor\n",
    "meta_regressor_1, test_data_1 = trainMetaRegressor(train_10, train_25, train_100, p_10, p_25, p_100, predictor_metrics, coverLog, all_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the response and prediction\n",
    "response = ['regress']\n",
    "remove_variables = coverLog + response\n",
    "# Inverse transform the response\n",
    "cover_unscaled = y_scaler.inverse_transform(test_data_1[coverLog])\n",
    "# Inverse transform the prediction\n",
    "predict_unscaled = y_scaler.inverse_transform(test_data_1[response])\n",
    "# Add unscaled data to data frame\n",
    "test_data_1.drop(labels=remove_variables, axis='columns', inplace=True)\n",
    "test_data_1 = pd.concat([test_data_1, pd.DataFrame(data=cover_unscaled, columns=coverLog)], axis=1)\n",
    "test_data_1 = pd.concat([test_data_1, pd.DataFrame(data=predict_unscaled, columns=response)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the predicted test data\n",
    "output_file = 'E:/VegetationEcology/Data_Harmonization/Project_GIS/Data_Output/testRegression/predicted.csv'\n",
    "test_data_1.to_csv(output_file, header=True, index=False, sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
