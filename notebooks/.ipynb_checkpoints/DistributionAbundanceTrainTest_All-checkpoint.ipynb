{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution-abundance Train and Test (All -> AIM)\n",
    "\n",
    "**Written by Timm Nawrocki**\n",
    "\n",
    "*Last updated Saturday, October 22, 2018.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# ---------------------------------------------------------------------------\n",
    "# Distribution-abundance Train and Test\n",
    "# Author: Timm Nawrocki, Alaska Center for Conservation Science\n",
    "# Created on: 2018-10-22\n",
    "# Usage: Must be executed as a Jupyter Notebook in an Anaconda 3 installation.\n",
    "# Description: \"Distribution-Abundance Train and Test\" trains a classifier to predict species presence and absence and trains a regressor to predict species abundance within areas of predicted presence. The predictions are composited into a single continuous output that can theoretically range from 0 to 100 representing percent cover. All model performance metrics are calculated on independent test partitions.\n",
    "# ---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "This script runs the model train and test steps to output a model performance and variable importance report, trained classifier file, trained regressor file, and threshold files that can be transferred to the predict script. The script is formatted as a Jupyter Notebook and is intended to be run on a Google Cloud virtual machine with 2 vCPUs and 13 GB of CPU memory with an Ubuntu operating system (18.04 LTS). The classifier and regressor in this script are set to use 1 core to provide the most cost efficient, rather than the fastest, model fit and prediction. For information on generating inputs for this script or on setting up Google Cloud virtual machines, see the [project readme](https://github.com/accs-uaa/vegetation-cover-modeling).\n",
    "\n",
    "The distribution and abundance of a species is two distinct problems: 1. Where does the species occur? 2. Where the species occurs, how much is present? To represent the nested nature of the problems, we developed a hierarchical model where a classifier predicted species presence-absence and a regressor predicted species abundance in areas of predicted species presence. The practical advantage to this hierarchical method is that it accurately predicts zeros (i.e., absences), which are of disproportionate ecological value than exact prediction of other values along the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Data and Variables\n",
    "\n",
    "This script relies on data that has been pre-processed into a csv file using the \"Format Taxon Data\" ArcGIS Pro script tool. The csv file must include all presence and absence observations for the target taxon or aggregate and all values for features extracted to the site locations. Features defined below must be modified to match the input csv file if changes to the construction of features are made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input file\n",
    "input_file = 'E:/VegetationEcology/Data_Harmonization/Project_GIS/Data_Output/speciesData/salix_pulchra.csv'\n",
    "# Define output folder\n",
    "output_folder = 'E:/VegetationEcology/Data_Harmonization/Project_GIS/Data_Output/modelResults/test/'\n",
    "# Define output report\n",
    "output_report_name = 'salix-pulchra-report.html'\n",
    "# Define species, genera, or aggregate name\n",
    "taxon_name = 'Salix pulchra'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variable sets\n",
    "predictor_all = ['compoundTopographic', 'dateFreeze_2000s', 'dateThaw_2000s', 'elevation', 'floodplainsDist', 'growingSeason_2000s', 'heatLoad', 'integratedMoisture', 'precipAnnual_2000s', 'roughness', 'siteExposure', 'slope', 'streamLargeDist', 'streamSmallDist', 'summerWarmth_2000s', 'surfaceArea', 'surfaceRelief', 'aspect', 'may_1_ultraBlue', 'may_2_blue', 'may_3_green', 'may_4_red', 'may_5_nearInfrared', 'may_6_shortInfrared1', 'may_7_shortInfrared2', 'may_evi2', 'may_nbr', 'may_ndmi', 'may_ndsi', 'may_ndvi', 'may_ndwi', 'june_1_ultraBlue', 'june_2_blue', 'june_3_green', 'june_4_red', 'june_5_nearInfrared', 'june_6_shortInfrared1', 'june_7_shortInfrared2', 'june_evi2', 'june_nbr', 'june_ndmi', 'june_ndsi', 'june_ndvi', 'june_ndwi', 'july_1_ultraBlue', 'july_2_blue', 'july_3_green', 'july_4_red', 'july_5_nearInfrared', 'july_6_shortInfrared1', 'july_7_shortInfrared2', 'july_evi2', 'july_nbr', 'july_ndmi', 'july_ndsi', 'july_ndvi', 'july_ndwi', 'august_1_ultraBlue', 'august_2_blue', 'august_3_green', 'august_4_red', 'august_5_nearInfrared', 'august_6_shortInfrared1', 'august_7_shortInfrared2', 'august_evi2', 'august_nbr', 'august_ndmi', 'august_ndsi', 'august_ndvi', 'august_ndwi', 'september_1_ultraBlue', 'september_2_blue', 'september_3_green', 'september_4_red', 'september_5_nearInfrared', 'september_6_shortInfrared1', 'september_7_shortInfrared2', 'september_evi2', 'september_nbr', 'september_ndmi', 'september_ndsi', 'september_ndvi', 'september_ndwi']\n",
    "zero_variable = ['zero']\n",
    "strata = ['strata']\n",
    "cover = ['cover']\n",
    "retain_variables = ['project', 'siteID', 'siteCode', 'methodSurvey', 'methodCover', 'plotDimensions', 'vascularScope', 'nonvascularScope', 'lichenScope', 'date', 'datum', 'latitude', 'longitude', 'ten', 'twentyfive']\n",
    "coordinates = ['POINT_X', 'POINT_Y']\n",
    "all_variables = retain_variables + coordinates + predictor_all + zero_variable + strata + cover\n",
    "iteration = ['iteration']\n",
    "absence = ['absence']\n",
    "presence = ['presence']\n",
    "response = ['response']\n",
    "prediction = ['prediction']\n",
    "output_variables = all_variables + iteration + absence + presence + response + prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize\n",
    "\n",
    "This script has general dependencies on the *os* package for file system manipulations, the *numpy* and *pandas* packages for data manipulations, and the *seaborn* and *matplotlib* packages for plotting. *GPy* and *GPyOpt* are Gaussian Process packages that drive the bayesian optimization of hyperparameters. *XGBoost* provides the gradient boosting classifier and regressor used to create the composite predictions. *Scikit Learn* provides a random forest classifier and regressor for comparison to the gradient boosting performance, model selection and cross validation tools, performance metrics, and a function to save models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages for file manipulation, data manipulation, and plotting\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plot\n",
    "# Import module for altering output display\n",
    "from IPython.display import clear_output\n",
    "# Import packages for bayesian optimization\n",
    "import GPy\n",
    "import GPyOpt\n",
    "from GPyOpt.methods import BayesianOptimization\n",
    "# Import XGBoost gradient boosting implementations\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import XGBRegressor\n",
    "# Import modules for model selection, cross validation, random forest, and performance from Scikit Learn\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a plots folder if it does not exist\n",
    "plots_folder = os.path.join(output_folder, \"plots\")\n",
    "if not os.path.exists(plots_folder):\n",
    "    os.makedirs(plots_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output test data\n",
    "output_csv = os.path.join(output_folder, 'prediction.csv')\n",
    "# Define output model files\n",
    "output_classifier = os.path.join(output_folder, 'classifier.joblib')\n",
    "output_regressor = os.path.join(output_folder, 'regressor.joblib')\n",
    "# Define output threshold file\n",
    "threshold_file = os.path.join(output_folder, 'threshold.txt')\n",
    "# Define threshold and metrics table\n",
    "metrics_file = os.path.join(output_folder, 'metrics.csv')\n",
    "# Define output feature elimination plots\n",
    "classifier_feature_file = os.path.join(plots_folder, \"classifier_feature_elimination.png\")\n",
    "regressor_feature_file = os.path.join(plots_folder, \"regressor_feature_elimination.png\")\n",
    "# Define output correlation plot\n",
    "variable_correlation = os.path.join(plots_folder, \"variable_correlation.png\")\n",
    "# Define output variable importance plots\n",
    "importance_classifier = os.path.join(plots_folder, \"importance_classifier.png\")\n",
    "importance_regressor = os.path.join(plots_folder, \"importance_regressor.png\")\n",
    "# Define output bayesian optimization convergence plots\n",
    "convergence_classifier = os.path.join(plots_folder, \"convergence_classifier.png\")\n",
    "convergence_regressor = os.path.join(plots_folder, \"convergence_regressor.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Functions\n",
    "\n",
    "Analyses are conducted in units represented by functions. The functions are defined below in order of use. In general, functions in this script fall into three categories: Bayesian Optimization, Train and Test Iteration, and Export Results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Bayesian Optimization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an optimization objective function for the xgboost classifier\n",
    "def cvClassifier(parameters):\n",
    "    # Define the search parameter set\n",
    "    parameters = parameters[0]\n",
    "    # Create a cross validation split method with 10 splits\n",
    "    cv_splits = ShuffleSplit(n_splits=10, test_size=0.3, train_size=0.7, random_state=None)\n",
    "    # Define the cross validator\n",
    "    score = cross_val_score(\n",
    "        XGBClassifier(max_depth=int(parameters[0]),\n",
    "                     learning_rate=parameters[1],\n",
    "                     n_estimators=int(parameters[2]),\n",
    "                     silent=True,\n",
    "                     objective='binary:logistic',\n",
    "                     booster='gbtree',\n",
    "                     n_jobs=1,\n",
    "                     gamma=parameters[3],\n",
    "                     min_child_weight=int(parameters[4]),\n",
    "                     max_delta_step=int(parameters[5]),\n",
    "                     subsample=parameters[6],\n",
    "                     colsample_bytree=parameters[7],\n",
    "                     colsample_bylevel=parameters[8],\n",
    "                     reg_alpha=parameters[9],\n",
    "                     reg_lambda=parameters[10],\n",
    "                     scale_pos_weight=parameters[11]),\n",
    "        X, y, scoring='roc_auc', cv=cv_splits).mean()\n",
    "    # Convert the mean score to array and return the inverse of the array for minimization\n",
    "    score = np.array(score)\n",
    "    return -score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an optimization objective function for the xgboost regressor\n",
    "def cvRegressor(parameters):\n",
    "    # Define the search parameter set\n",
    "    parameters = parameters[0]\n",
    "    # Create a cross validation split method with 100 splits\n",
    "    cv_splits = ShuffleSplit(n_splits=100, test_size=0.3, train_size=0.7, random_state=None)\n",
    "    # Define the cross validator\n",
    "    score = cross_val_score(\n",
    "        XGBRegressor(max_depth=int(parameters[0]),\n",
    "                     learning_rate=parameters[1],\n",
    "                     n_estimators=int(parameters[2]),\n",
    "                     silent=True,\n",
    "                     objective='reg:linear',\n",
    "                     booster='gbtree',\n",
    "                     n_jobs=1,\n",
    "                     gamma=parameters[3],\n",
    "                     min_child_weight=int(parameters[4]),\n",
    "                     max_delta_step=int(parameters[5]),\n",
    "                     subsample=parameters[6],\n",
    "                     colsample_bytree=parameters[7],\n",
    "                     colsample_bylevel=parameters[8],\n",
    "                     reg_alpha=parameters[9],\n",
    "                     reg_lambda=parameters[10],\n",
    "                     scale_pos_weight=parameters[11]),\n",
    "        X, y, scoring='r2', cv=cv_splits).mean()\n",
    "    # Convert the mean score to array and return the inverse of the array for minimization\n",
    "    score = np.array(score)\n",
    "    return -score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an optimization function\n",
    "def bayesianOptimizer(objective_function, initial, iterations, X, y, plot_file):\n",
    "    # Create the hyperparameter search domain\n",
    "    domain=[{'name': 'max_depth', 'type': 'discrete', 'domain': (3, 10)},\n",
    "            {'name': 'learning_rate', 'type': 'continuous', 'domain': (0, 1)},\n",
    "            {'name': 'n_estimators', 'type': 'discrete', 'domain': (500, 5000)},\n",
    "            {'name': 'gamma', 'type': 'continuous', 'domain': (0, 5)},\n",
    "            {'name': 'min_child_weight', 'type': 'discrete', 'domain': (0, 10)},\n",
    "            {'name': 'max_delta_step', 'type': 'continuous', 'domain': (0, 5)},\n",
    "            {'name': 'subsample', 'type': 'continuous', 'domain': (0.5, 1)},\n",
    "            {'name': 'colsample_bytree', 'type': 'continuous', 'domain': (0.3, 1)},\n",
    "            {'name': 'colsample_bylevel', 'type': 'continuous', 'domain': (0.3, 1)},\n",
    "            {'name': 'reg_alpha', 'type': 'continuous', 'domain': (0, 10)},\n",
    "            {'name': 'reg_lamda', 'type': 'continuous', 'domain': (0, 10)},\n",
    "            {'name': 'scale_pos_weight', 'type': 'continuous', 'domain': (0, 5)}]\n",
    "    # Initialize the Bayesian Optimizer\n",
    "    optimizer = BayesianOptimization(f=objective_function,\n",
    "                                     domain=domain,\n",
    "                                     model_type='GP',\n",
    "                                     initial_design_numdata=initial,\n",
    "                                     initial_design_type='random',\n",
    "                                     acquisition_type ='EI',\n",
    "                                     exact_feval=False,\n",
    "                                     maximize=False)\n",
    "    # Run 250 iterations of optimization\n",
    "    optimizer.run_optimization(max_iter=iterations)\n",
    "    # Plot the convergence of the best solution\n",
    "    optimizer.plot_convergence(filename=plot_file)\n",
    "    # Return results\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Train and Test Iteration Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to create the train and test partitions\n",
    "def partitionData(inData, all_variables, predictors, response, strata):\n",
    "    # Create train and test splits\n",
    "    X = inData[all_variables]\n",
    "    y = inData[response[0]]\n",
    "    stratify = inData[strata[0]]\n",
    "    all_train, all_test, y_train, y_test = train_test_split(X,\n",
    "                                                            y,\n",
    "                                                            test_size = 0.3,\n",
    "                                                            train_size = 0.7,\n",
    "                                                            random_state = None,\n",
    "                                                            shuffle = True,\n",
    "                                                            stratify = stratify)\n",
    "    # Reset the index on the train and test splits\n",
    "    all_train = all_train.reset_index()\n",
    "    all_test = all_test.reset_index()\n",
    "    return all_train, all_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to train a classifier on the train X and y and predict the test X\n",
    "def trainPredictClassifier(train, test, predictors, response, parameters):\n",
    "    # Define X and y\n",
    "    X_train = train[predictors]\n",
    "    y_train = train[response[0]]\n",
    "    X_test = test[predictors]\n",
    "    # Fit a classifier to the train X and y\n",
    "    classifier = XGBClassifier(max_depth=int(parameters[0]),\n",
    "                               learning_rate=parameters[1],\n",
    "                               n_estimators=int(parameters[2]),\n",
    "                               silent=True,\n",
    "                               objective='binary:logistic',\n",
    "                               booster='gbtree',\n",
    "                               n_jobs=1,\n",
    "                               gamma=parameters[3],\n",
    "                               min_child_weight=int(parameters[4]),\n",
    "                               max_delta_step=int(parameters[5]),\n",
    "                               subsample=parameters[6],\n",
    "                               colsample_bytree=parameters[7],\n",
    "                               colsample_bylevel=parameters[8],\n",
    "                               reg_alpha=parameters[9],\n",
    "                               reg_lambda=parameters[10],\n",
    "                               scale_pos_weight=parameters[11])\n",
    "    classifier.fit(X_train, y_train)\n",
    "    # Use the classifier to predict class probabilities\n",
    "    prediction = classifier.predict_proba(X_test)\n",
    "    # Concatenate predicted values to test data frame\n",
    "    test = pd.concat([test, pd.DataFrame(prediction)], axis=1)\n",
    "    test = test.rename(index=int, columns={0: 'absence', 1: 'presence'})\n",
    "    # Return the predictions\n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to train a regressor on the train X and y and predict the test X\n",
    "def trainPredictRegressor(train, test, predictors, response, parameters):\n",
    "    # Define X and y\n",
    "    X_train = train[predictors]\n",
    "    y_train = train[response[0]]\n",
    "    X_test = test[predictors]\n",
    "    # Fit a regressor to the train X and y\n",
    "    regressor = XGBRegressor(max_depth=int(parameters[0]),\n",
    "                             learning_rate=parameters[1],\n",
    "                             n_estimators=int(parameters[2]),\n",
    "                             silent=True,\n",
    "                             objective='reg:linear',\n",
    "                             booster='gbtree',\n",
    "                             n_jobs=1,\n",
    "                             gamma=parameters[3],\n",
    "                             min_child_weight=int(parameters[4]),\n",
    "                             max_delta_step=int(parameters[5]),\n",
    "                             subsample=parameters[6],\n",
    "                             colsample_bytree=parameters[7],\n",
    "                             colsample_bylevel=parameters[8],\n",
    "                             reg_alpha=parameters[9],\n",
    "                             reg_lambda=parameters[10],\n",
    "                             scale_pos_weight=parameters[11])\n",
    "    regressor.fit(X_train, y_train)\n",
    "    # Use the regressor to predict response values\n",
    "    prediction = regressor.predict(X_test)\n",
    "    # Concatenate predicted values to test data frame\n",
    "    test = pd.concat([test, pd.DataFrame(prediction)], axis=1)\n",
    "    test = test.rename(index=int, columns={0: 'response'})\n",
    "    # Return the predictions\n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate performance metrics based on a specified threshold value\n",
    "def testPresenceThreshold(predict_probability, threshold, y_test):\n",
    "    # Create an empty array of zeroes that matches the length of the probability predictions\n",
    "    predict_thresholded = np.zeros(predict_probability.shape)\n",
    "    # Set values for all probabilities greater than or equal to the threshold equal to 1\n",
    "    predict_thresholded[predict_probability >= threshold] = 1\n",
    "    # Determine error rates\n",
    "    confusion_test = confusion_matrix(y_test, predict_thresholded)\n",
    "    true_negative = confusion_test[0,0]\n",
    "    false_negative = confusion_test[1,0]\n",
    "    true_positive = confusion_test[1,1]\n",
    "    false_positive = confusion_test[0,1]\n",
    "    # Calculate sensitivity and specificity\n",
    "    sensitivity = true_positive / (true_positive + false_negative)\n",
    "    specificity = true_negative / (true_negative + false_positive)\n",
    "    # Calculate AUC score\n",
    "    auc = roc_auc_score(y_test, predict_probability)\n",
    "    # Calculate overall accuracy\n",
    "    accuracy = (true_negative + true_positive) / (true_negative + false_positive + false_negative + true_positive)\n",
    "    # Return the thresholded probabilities and the performance metrics\n",
    "    return (sensitivity, specificity, auc, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to determine a presence threshold\n",
    "def determineOptimalThreshold(predict_probability, y_test):\n",
    "    # Iterate through numbers between 0 and 1000 to output a list of sensitivity and specificity values per threshold number\n",
    "    i = 1\n",
    "    sensitivity_list = []\n",
    "    specificity_list = []\n",
    "    while i < 1001:\n",
    "        threshold = i/1000\n",
    "        sensitivity, specificity, auc, accuracy = testPresenceThreshold(predict_probability, threshold, y_test)\n",
    "        sensitivity_list.append(sensitivity)\n",
    "        specificity_list.append(specificity)\n",
    "        i = i + 1\n",
    "    # Calculate a list of absolute value difference between sensitivity and specificity and find the optimal threshold\n",
    "    difference_list = [np.absolute(a - b) for a, b in zip(sensitivity_list, specificity_list)]\n",
    "    value, threshold = min((value, threshold) for (threshold, value) in enumerate(difference_list))\n",
    "    threshold = threshold/1000\n",
    "    # Calculate the performance of the optimal threshold\n",
    "    sensitivity, specificity, auc, accuracy = testPresenceThreshold(predict_probability, threshold, y_test)\n",
    "    # Return the optimal threshold and the performance metrics of the optimal threshold\n",
    "    return threshold, sensitivity, specificity, auc, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to composite model results\n",
    "def compositePrediction(test, presence, response, threshold):\n",
    "    # Define a function to threshold absences and set presences equal to regression response\n",
    "    def compositeRows(row):\n",
    "        if row[presence[0]] < threshold:\n",
    "            return 0\n",
    "        elif row[presence[0]] >= threshold:\n",
    "            return row[response[0]]\n",
    "    # Apply function to all rows in test data\n",
    "    test['prediction'] = test.apply(lambda row: compositeRows(row), axis=1)\n",
    "    # Return the test data frame with composited results\n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to calculate pseudo r-squared and RMSE for the composited prediction\n",
    "def calculatePerformance(test, cover, prediction):\n",
    "    # Define the true values and the predicted values for the response variable\n",
    "    y_test = test[cover[0]]\n",
    "    y_prediction = test[prediction[0]]\n",
    "    # Calculate pseudo r-squared\n",
    "    r_score = r2_score(y_test, y_prediction, sample_weight=None, multioutput='uniform_average')\n",
    "    # Calculate error\n",
    "    mae = mean_absolute_error(y_test, y_prediction)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_prediction))\n",
    "    # Return performance metrics\n",
    "    return r_score, mae, rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Export Results Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results of the recursive feature eliminator for the classifier\n",
    "def plotFeatureElimination(feature_eliminator, y_label, feature_plot_file):\n",
    "    feature_plot = plot.figure()\n",
    "    fig_size = plot.rcParams[\"figure.figsize\"]\n",
    "    fig_size[0] = 8\n",
    "    fig_size[1] = 6\n",
    "    plot.rcParams[\"figure.figsize\"] = fig_size\n",
    "    plot.xlabel(\"Number of features selected\")\n",
    "    plot.ylabel(y_label)\n",
    "    plot.plot(range(1, len(feature_eliminator.grid_scores_) + 1), feature_eliminator.grid_scores_)\n",
    "    plot.show()\n",
    "    feature_plot.savefig(feature_plot_file, bbox_inches=\"tight\", dpi=300)\n",
    "    # Clear plot workspace\n",
    "    plot.clf()\n",
    "    plot.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot Pearson correlation of predictor variables\n",
    "def plotVariableCorrelation(X_train, outFile):\n",
    "    # Calculate Pearson correlation coefficient between the predictor variables, where -1 is perfect negative correlation and 1 is perfect positive correlation\n",
    "    correlation = X_train.astype('float64').corr()\n",
    "    # Generate a mask for the upper triangle of plot\n",
    "    mask = np.zeros_like(correlation, dtype=np.bool)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "    # Set up the matplotlib figure\n",
    "    f, ax = plot.subplots(figsize=(20, 18))\n",
    "    # Generate a custom diverging colormap\n",
    "    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "    # Draw the heatmap with the mask and correct aspect ratio\n",
    "    correlation_plot = sns.heatmap(correlation, mask=mask, cmap=cmap, vmax=.3, center=0, square=True, linewidths=.5, cbar_kws={'shrink': .5})\n",
    "    correlation_figure = correlation_plot.get_figure()\n",
    "    correlation_figure.savefig(outFile, bbox_inches='tight', dpi=300)\n",
    "    # Clear plot workspace\n",
    "    plot.clf()\n",
    "    plot.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot variable importances\n",
    "def plotVariableImportances(inModel, X_train, outVariableFile):\n",
    "    # Get numerical feature importances\n",
    "    importances = list(inModel.feature_importances_)\n",
    "    # List of tuples with variable and importance\n",
    "    feature_list = list(X_train.columns)\n",
    "    feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "    # Sort the feature importances by most important first\n",
    "    feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "    # Initialize the plot and set figure size\n",
    "    variable_figure = plot.figure()\n",
    "    fig_size = plot.rcParams[\"figure.figsize\"]\n",
    "    fig_size[0] = 18\n",
    "    fig_size[1] = 6\n",
    "    plot.rcParams[\"figure.figsize\"] = fig_size\n",
    "    # Create list of x locations for plotting\n",
    "    x_values = list(range(len(importances)))\n",
    "    # Make a bar chart of the variable importances\n",
    "    plot.bar(x_values, importances, orientation = 'vertical')\n",
    "    # Tick labels for x axis\n",
    "    plot.xticks(x_values, feature_list, rotation='vertical')\n",
    "    # Axis labels and title\n",
    "    plot.ylabel('Importance'); plot.xlabel('Variable'); plot.title('Variable Importances');\n",
    "    # Export\n",
    "    variable_figure.savefig(outVariableFile, bbox_inches=\"tight\", dpi=300)\n",
    "    # Clear plot workspace\n",
    "    plot.clf()\n",
    "    plot.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to train and export a classifier\n",
    "def trainExportClassifier(input_data, predictors, response, parameters, outModel, outImportance):\n",
    "    # Define the predictor labels (X) and the response label (y) in the input dataframe\n",
    "    X = input_data[predictors]\n",
    "    y = input_data[response[0]]\n",
    "    # Fit a classifier to the input dataset\n",
    "    classifier = XGBClassifier(max_depth=int(parameters[0]),\n",
    "                               learning_rate=parameters[1],\n",
    "                               n_estimators=int(parameters[2]),\n",
    "                               silent=True,\n",
    "                               objective='binary:logistic',\n",
    "                               booster='gbtree',\n",
    "                               n_jobs=1,\n",
    "                               gamma=parameters[3],\n",
    "                               min_child_weight=int(parameters[4]),\n",
    "                               max_delta_step=int(parameters[5]),\n",
    "                               subsample=parameters[6],\n",
    "                               colsample_bytree=parameters[7],\n",
    "                               colsample_bylevel=parameters[8],\n",
    "                               reg_alpha=parameters[9],\n",
    "                               reg_lambda=parameters[10],\n",
    "                               scale_pos_weight=parameters[11])\n",
    "    classifier.fit(X, y)\n",
    "    # Save classifier to an external file\n",
    "    joblib.dump(classifier, outModel)\n",
    "    # Export a variable importance plot\n",
    "    plotVariableImportances(classifier, X, outImportance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainExportRegressor(input_data, predictors, response, parameters, outModel, outImportance):\n",
    "    # Define the predictor labels (X) and the response label (y) in the input dataframe\n",
    "    X = input_data[predictors]\n",
    "    y = input_data[response[0]]\n",
    "    # Fit a classifier to the input dataset\n",
    "    regressor = XGBRegressor(max_depth=int(parameters[0]),\n",
    "                             learning_rate=parameters[1],\n",
    "                             n_estimators=int(parameters[2]),\n",
    "                             silent=True,\n",
    "                             objective='reg:linear',\n",
    "                             booster='gbtree',\n",
    "                             n_jobs=1,\n",
    "                             gamma=parameters[3],\n",
    "                             min_child_weight=int(parameters[4]),\n",
    "                             max_delta_step=int(parameters[5]),\n",
    "                             subsample=parameters[6],\n",
    "                             colsample_bytree=parameters[7],\n",
    "                             colsample_bylevel=parameters[8],\n",
    "                             reg_alpha=parameters[9],\n",
    "                             reg_lambda=parameters[10],\n",
    "                             scale_pos_weight=parameters[11])\n",
    "    regressor.fit(X, y)\n",
    "    # Save classifier to an external file\n",
    "    joblib.dump(regressor, outModel)\n",
    "    # Export a variable importance plot\n",
    "    plotVariableImportances(regressor, X, outImportance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conduct Analyses\n",
    "\n",
    "The analyses are subdivided into subsections: load data, assess untuned model performance, recursive feature elimination, bayesian optimization, train and test iterations, and export results. The majority of analytical time is devoted to recursive feature elimination and bayesian optimization to ensure selection of a best performing model. The XGBoost implementation of stochastic gradient boosting is the primary algorithm. Random forest is included in the assessment of untuned model performance for comparison with stochastic gradient boosting.\n",
    "\n",
    "The optimal feature set and hyperparameters are passed into 100 iterations of training and testing in which the results of the classifier and regressor are composited to determine overall performance. For each iteration, we withhold 30% of the data at random stratified by broad cover ranges as an independent test partition. The composited prediction results are appended into a single data frame for additional analyses and plotting external to this script.\n",
    "\n",
    "Outputs of the analysis are:\n",
    "1. Report of performance, including the following plots:\n",
    "  1. Recursive feature elimination for the classifier\n",
    "  2. Recursive feature elimination for the regressor\n",
    "  3. Convergence of the classifier hyperparameters\n",
    "  4. Convergence of the regressor hyperparameters\n",
    "  5. Variable importances of the classifier\n",
    "  6. Variable importances of the regressor\n",
    "  7. Pearson correlation for all predictors\n",
    "2. Model files:\n",
    "  1. Classifier\n",
    "  2. Regressor\n",
    "3. Table of performance metrics\n",
    "4. Mean threshold\n",
    "5. Test predictions from 100 validation iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Load Data\n",
    "\n",
    "Two data instances were created based on the input csv file: an input data frame and a presence only AIM NPR-A data subset. The presence-absence classifiers were trained from the input data and the abundance regressors were trained from the presence AIM data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data frame of input data\n",
    "input_data = pd.read_csv(input_file)\n",
    "# Convert values to floats\n",
    "input_data[predictor_all + cover + coordinates] = input_data[predictor_all + cover + coordinates].astype(float)\n",
    "# Convert values to integers\n",
    "input_data[strata + zero_variable] = input_data[strata + zero_variable].astype(int)\n",
    "# Shuffle data\n",
    "input_data = shuffle(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset the training data to presences from AIM NPR-A\n",
    "presence_data = input_data[input_data[cover[0]] >= 1]\n",
    "presence_data = presence_data[presence_data['project'] == 'AIM NPR-A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set initial plot sizefig_size = plot.rcParams[\"figure.figsize\"]\n",
    "fig_size = plot.rcParams[\"figure.figsize\"]\n",
    "fig_size[0] = 8\n",
    "fig_size[1] = 6\n",
    "plot.rcParams[\"figure.figsize\"] = fig_size\n",
    "plot.style.use('grayscale')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Assess untuned model performance\n",
    "\n",
    "For comparison, the untuned performance of a gradient boosting and random forest classifier and regressor are assessed by shuffle split cross validation. The performance of the final model should be comparable to or better than the performance of the untuned models. The performance of the classifier has low variance and is therefore cross validated with only 10 iterations. The test split is retained at 30% of the data for all cross validation and test purposes in this script. The performance of the regressor has high variance, likely derived from the low sample size, and is therefore cross validated with 100 iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.1. Untuned classifier performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a cross validation split method\n",
    "cv_splits_classifier = ShuffleSplit(n_splits=10, test_size=0.3, train_size=0.7, random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the X and y data\n",
    "X = input_data[predictor_all]\n",
    "y = input_data[zero_variable[0]]\n",
    "# Shuffle data\n",
    "X, y = shuffle(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random forest classifier with 5000 trees\n",
    "rfClassifier = RandomForestClassifier(n_estimators=5000,\n",
    "                                      criterion='entropy',\n",
    "                                      max_features='log2',\n",
    "                                      bootstrap=True,\n",
    "                                      oob_score=False,\n",
    "                                      n_jobs=1,\n",
    "                                      class_weight='balanced')\n",
    "# Test the performance of the random forest classifier using 5-fold cross validation\n",
    "baseline_rfClassifier = cross_val_score(rfClassifier, X, y, scoring='roc_auc', cv=cv_splits_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the mean and standard deviation of AUC for the random forest classifier\n",
    "print(baseline_rfClassifier.mean())\n",
    "print(baseline_rfClassifier.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an xgboost classifier with untuned settings\n",
    "xgbClassifier = XGBClassifier(max_depth=3,\n",
    "                              learning_rate=0.05,\n",
    "                              n_estimators=5000,\n",
    "                              silent=True,\n",
    "                              objective='binary:logistic',\n",
    "                              booster='gbtree',\n",
    "                              n_jobs=1,\n",
    "                              gamma=3,\n",
    "                              min_child_weight=2,\n",
    "                              max_delta_step=1,\n",
    "                              subsample=0.8,\n",
    "                              colsample_bytree=0.8,\n",
    "                              colsample_bylevel=0.5,\n",
    "                              reg_alpha=2,\n",
    "                              reg_lambda=2,\n",
    "                              scale_pos_weight=1)\n",
    "# Test the perfomance of the default xgboost classifier using 5-fold cross validation\n",
    "baseline_xgbClassifier = cross_val_score(xgbClassifier, X, y, scoring='roc_auc', cv=cv_splits_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the mean and standard deviation of AUC for the untuned xgboost classifier\n",
    "print(baseline_xgbClassifier.mean())\n",
    "print(baseline_xgbClassifier.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.2. Untuned regressor performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a cross validation split method\n",
    "cv_splits_regressor = ShuffleSplit(n_splits=100, test_size=0.3, train_size=0.7, random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the X and y data\n",
    "X = presence_data[predictor_all]\n",
    "y = presence_data[cover[0]]\n",
    "# Shuffle data\n",
    "X, y = shuffle(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random forest regressor with 5000 trees\n",
    "rfRegressor = RandomForestRegressor(n_estimators=5000,\n",
    "                                    criterion='mse',\n",
    "                                    bootstrap=True,\n",
    "                                    oob_score=False,\n",
    "                                    n_jobs=1)\n",
    "# Test the performance of the random forest regressor using 5-fold cross validation\n",
    "baseline_rfRegressor = cross_val_score(rfRegressor, X, y, scoring='r2', cv=cv_splits_regressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the mean and standard deviation of R Squared for the random forest regressor\n",
    "print(baseline_rfRegressor.mean())\n",
    "print(baseline_rfRegressor.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an xgboost regressor with untuned settings\n",
    "xgbRegressor = XGBRegressor(max_depth=3,\n",
    "                            learning_rate=0.05,\n",
    "                            n_estimators=5000,\n",
    "                            silent=True,\n",
    "                            objective='reg:linear',\n",
    "                            booster='gbtree',\n",
    "                            n_jobs=1,\n",
    "                            gamma=3.3,\n",
    "                            min_child_weight=2,\n",
    "                            max_delta_step=1,\n",
    "                            subsample=0.8,\n",
    "                            colsample_bytree=0.5,\n",
    "                            colsample_bylevel=0.5,\n",
    "                            reg_alpha=2,\n",
    "                            reg_lambda=6,\n",
    "                            scale_pos_weight=1)\n",
    "# Test the performance of the default xgboost regressor using 5-fold cross validation\n",
    "baseline_xgbRegressor = cross_val_score(xgbRegressor, X, y, scoring='r2', cv=cv_splits_regressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the mean and standard deviation of R Squared for the untuned xgboost regressor\n",
    "print(baseline_xgbRegressor.mean())\n",
    "print(baseline_xgbRegressor.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Recursive Feature Elimination\n",
    "\n",
    "The importance of features vary by species and response type. Some features may be uninformative for some species/response combinations, increasing the noise in the prediction. We used a data-driven approach to reduce the features to the best performing subset by implementing recursive feature selection with cross validation. The classifier features were selected on the maximization of ROC AUC using 10 cross validation splits. The regressor features were selected on the maximization of R Squared using 100 cross validation splits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.1. Classifier Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the X and y data\n",
    "X = input_data[predictor_all]\n",
    "y = input_data[zero_variable]\n",
    "# Shuffle data\n",
    "X, y = shuffle(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the classifier used to generate variable importances\n",
    "classifier = XGBClassifier(max_depth=3,\n",
    "                           learning_rate=0.05,\n",
    "                           n_estimators=5000,\n",
    "                           silent=True,\n",
    "                           objective='binary:logistic',\n",
    "                           booster='gbtree',\n",
    "                           n_jobs=1,\n",
    "                           gamma=3,\n",
    "                           min_child_weight=2,\n",
    "                           max_delta_step=1,\n",
    "                           subsample=0.8,\n",
    "                           colsample_bytree=0.8,\n",
    "                           colsample_bylevel=0.5,\n",
    "                           reg_alpha=2,\n",
    "                           reg_lambda=2,\n",
    "                           scale_pos_weight=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a recursive feature eliminator for the classifier using shuffled cross validation\n",
    "recursive_eliminator_classifier = RFECV(estimator=classifier, step=1, cv=cv_splits_classifier, scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the recursive feature eliminator\n",
    "recursive_eliminator_classifier.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print optimal number of features for the classifier\n",
    "print(\"Optimal number of features : %d\" % recursive_eliminator_classifier.n_features_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results of the recursive feature eliminator for the classifier\n",
    "y_label = \"Cross validation score (ROC AUC)\"\n",
    "plotFeatureElimination(recursive_eliminator_classifier, y_label, classifier_feature_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the optimal subset of features for the classifier\n",
    "classifier_features = pd.DataFrame({'predictor': list(X.columns),\n",
    "                                    'select': recursive_eliminator_classifier.ranking_.tolist()})\n",
    "classifier_features = classifier_features.loc[classifier_features['select'] == 1]\n",
    "classifier_features = classifier_features['predictor'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.2. Regressor Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the X and y data\n",
    "X = presence_data[predictor_all]\n",
    "y = presence_data[cover]\n",
    "# Shuffle data\n",
    "X, y = shuffle(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the regressor used to generate variable importances\n",
    "regressor = XGBRegressor(max_depth=3,\n",
    "                         learning_rate=0.05,\n",
    "                         n_estimators=5000,\n",
    "                         silent=True,\n",
    "                         objective='reg:linear',\n",
    "                         booster='gbtree',\n",
    "                         n_jobs=1,\n",
    "                         gamma=3.3,\n",
    "                         min_child_weight=2,\n",
    "                         max_delta_step=1,\n",
    "                         subsample=0.8,\n",
    "                         colsample_bytree=0.5,\n",
    "                         colsample_bylevel=0.5,\n",
    "                         reg_alpha=2,\n",
    "                         reg_lambda=6,\n",
    "                         scale_pos_weight=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a recursive feature eliminator for the regressor using shuffled cross-validation\n",
    "recursive_eliminator_regressor = RFECV(estimator=regressor, step=1, cv=cv_splits_regressor, scoring='r2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the recursive feature eliminator\n",
    "recursive_eliminator_regressor.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print optimal number of features for the classifier\n",
    "print(\"Optimal number of features : %d\" % recursive_eliminator_regressor.n_features_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results of the recursive feature eliminator for the regressor\n",
    "y_label = \"Cross validation score (R Squared)\"\n",
    "plotFeatureElimination(recursive_eliminator_regressor, y_label, regressor_feature_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the optimal subset of features for the regressor\n",
    "regressor_features = pd.DataFrame({'predictor': list(X.columns),\n",
    "                                   'select': recursive_eliminator_regressor.ranking_.tolist()})\n",
    "regressor_features = regressor_features.loc[regressor_features['select'] == 1]\n",
    "regressor_features = regressor_features['predictor'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. Bayesian Optimization\n",
    "\n",
    "We used a Bayesian statistical framework with Gaussian Process as the generative model to find an optimal set of hyperparameters for the classifier and regressor. The Bayesian Optimization tuned species-specific models that make the best generalizations as estimated by cross validation. The domain space for bayesian optimization was large, so we initially sampled 50 points at random and then conducted 250 optimization iterations to converge on a best set of hyperparameters. Optimization occurred independently for the classifier and the regressor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4.1. Classifier optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set initial plot sizefig_size = plot.rcParams[\"figure.figsize\"]\n",
    "fig_size = plot.rcParams[\"figure.figsize\"]\n",
    "fig_size[0] = 24\n",
    "fig_size[1] = 12\n",
    "plot.rcParams[\"figure.figsize\"] = fig_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the X and y data\n",
    "X = input_data[classifier_features]\n",
    "y = input_data[zero_variable[0]]\n",
    "# Shuffle data\n",
    "X, y = shuffle(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Conduct bayesian optimization of xgboost classifier\n",
    "optimizer_classify = bayesianOptimizer(cvClassifier, 30, 170, X, y, convergence_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display highest AUC score achieved\n",
    "-np.amin(optimizer_classify.Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best set of parameters for the classifier\n",
    "classifier_parameters = optimizer_classify.X[np.argmin(optimizer_classify.Y)]\n",
    "# Display best parameters for the classifier\n",
    "classifier_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4.2. Regressor optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the X and y data\n",
    "X = presence_data[regressor_features]\n",
    "y = presence_data[cover]\n",
    "# Shuffle data\n",
    "X, y = shuffle(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct bayesian optimization of xgboost regressor\n",
    "optimizer_regress = bayesianOptimizer(cvRegressor, 50, 250, X, y, convergence_regressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display highest R Squared score achieved\n",
    "-np.amin(optimizer_regress.Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best set of hyperparameters for the regressor\n",
    "regressor_parameters = optimizer_regress.X[np.argmin(optimizer_regress.Y)]\n",
    "# Display best hyperparameters for the regressor\n",
    "regressor_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Train and Test Iterations (n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty lists to store threshold and performance metrics\n",
    "threshold_list = []\n",
    "sensitivity_list = []\n",
    "specificity_list = []\n",
    "auc_list = []\n",
    "accuracy_list = []\n",
    "overall_r_score_list = []\n",
    "overall_mae_list = []\n",
    "overall_rmse_list = []\n",
    "abundance_r_score_list = []\n",
    "abundance_mae_list = []\n",
    "abundance_rmse_list = []\n",
    "# Create an empty data frame to store the test results\n",
    "output_test = pd.DataFrame(columns=output_variables)\n",
    "i = 1\n",
    "while i < 101:\n",
    "    # Set output display to show one message with replacement\n",
    "    clear_output(wait=True)\n",
    "    # Split the data into train and test partitions with equal representation of AIM and non-AIM data\n",
    "    non_aim = input_data[input_data['project'] != 'AIM NPR-A']\n",
    "    aim = input_data[input_data['project'] == 'AIM NPR-A']\n",
    "    non_train, non_test = partitionData(non_aim, all_variables, predictor_all, zero_variable, strata)\n",
    "    aim_train, aim_test = partitionData(aim, all_variables, predictor_all, zero_variable, strata)\n",
    "    all_train = non_train.append(aim_train, ignore_index=True, sort=True)\n",
    "    all_test = non_test.append(aim_test, ignore_index=True, sort=True)\n",
    "    # Add the iteration count to the test data\n",
    "    all_test['iteration'] = i\n",
    "    # Train and predict a classifier\n",
    "    all_test = trainPredictClassifier(all_train, all_test, classifier_features, zero_variable, classifier_parameters)\n",
    "    # Subset the training data to presences from AIM NPR-A\n",
    "    presence_train = all_train[all_train[cover[0]] >= 1]\n",
    "    presence_train = presence_train[presence_train['project'] == 'AIM NPR-A']\n",
    "    # Train and predict a regressor\n",
    "    all_test = trainPredictRegressor(presence_train, all_test, regressor_features, cover, regressor_parameters)\n",
    "    # Calculate the optimal threshold and performance of the presence-absence classification\n",
    "    threshold, sensitivity, specificity, auc, accuracy = determineOptimalThreshold(all_test[presence[0]], all_test[zero_variable[0]])\n",
    "    # Composite the classifier and regressor predictions\n",
    "    all_test = compositePrediction(all_test, presence, response, threshold)\n",
    "    # Subset the test data to AIM NPR-A\n",
    "    aim_test = all_test[all_test['project'] == 'AIM NPR-A']\n",
    "    # Calculate overall performance\n",
    "    overall_r_score, overall_mae, overall_rmse = calculatePerformance(aim_test, cover, prediction)\n",
    "    # Subset the aim test to presences\n",
    "    aim_presences = aim_test[aim_test[cover[0]] >= 1]\n",
    "    # Calculate abundance performance\n",
    "    abundance_r_score, abundance_mae, abundance_rmse = calculatePerformance(aim_presences, cover, response)\n",
    "    # Add threshold and performance metrics to list\n",
    "    threshold_list.append(threshold)\n",
    "    sensitivity_list.append(sensitivity)\n",
    "    specificity_list.append(specificity)\n",
    "    auc_list.append(auc)\n",
    "    accuracy_list.append(accuracy)\n",
    "    overall_r_score_list.append(overall_r_score)\n",
    "    overall_mae_list.append(overall_mae)\n",
    "    overall_rmse_list.append(overall_rmse)\n",
    "    abundance_r_score_list.append(abundance_r_score)\n",
    "    abundance_mae_list.append(abundance_mae)\n",
    "    abundance_rmse_list.append(abundance_rmse)\n",
    "    # Add the test results to output data frame\n",
    "    output_test = output_test.append(all_test, ignore_index=True, sort=True)\n",
    "    print('Model train-test iteration ' + str(i) + ' out of 100 completed...')\n",
    "    # Increase the counter by 1\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export test results to csv\n",
    "output_test.to_csv(output_csv, header=True, index=False, sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set initial plot sizefig_size = plot.rcParams[\"figure.figsize\"]\n",
    "fig_size = plot.rcParams[\"figure.figsize\"]\n",
    "fig_size[0] = 16\n",
    "fig_size[1] = 12\n",
    "plot.rcParams[\"figure.figsize\"] = fig_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export a Pearson Correlation plot for the predictor variables\n",
    "plotVariableCorrelation(input_data[predictor_all], variable_correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set initial plot sizefig_size = plot.rcParams[\"figure.figsize\"]\n",
    "fig_size = plot.rcParams[\"figure.figsize\"]\n",
    "fig_size[0] = 18\n",
    "fig_size[1] = 6\n",
    "plot.rcParams[\"figure.figsize\"] = fig_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and export a final classifier using the full input data\n",
    "trainExportClassifier(input_data,\n",
    "                      classifier_features,\n",
    "                      zero_variable,\n",
    "                      classifier_parameters,\n",
    "                      output_classifier,\n",
    "                      importance_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and export a final regressor using the presence data from NPR-A AIM\n",
    "trainExportRegressor(presence_data,\n",
    "                     regressor_features,\n",
    "                     cover,\n",
    "                     regressor_parameters,\n",
    "                     output_regressor,\n",
    "                     importance_regressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean for threshold and all performance metrics\n",
    "threshold_mean = np.mean(threshold_list)\n",
    "sensitivity_mean = np.mean(sensitivity_list)\n",
    "specificity_mean = np.mean(specificity_list)\n",
    "auc_mean = np.mean(auc_list)\n",
    "accuracy_mean = np.mean(accuracy_list)\n",
    "overall_r_score_mean = np.mean(overall_r_score_list)\n",
    "overall_mae_mean = np.mean(overall_mae_list)\n",
    "overall_rmse_mean = np.mean(overall_rmse_list)\n",
    "abundance_r_score_mean = np.mean(abundance_r_score_list)\n",
    "abundance_mae_mean = np.mean(abundance_mae_list)\n",
    "abundance_rmse_mean = np.mean(abundance_rmse_list)\n",
    "# Calculate standard deviation for threshold and all performance metrics\n",
    "threshold_sd = np.std(threshold_list)\n",
    "sensitivity_sd = np.std(sensitivity_list)\n",
    "specificity_sd = np.std(specificity_list)\n",
    "auc_sd = np.std(auc_list)\n",
    "accuracy_sd = np.std(accuracy_list)\n",
    "overall_r_score_sd = np.std(overall_r_score_list)\n",
    "overall_mae_sd = np.std(overall_mae_list)\n",
    "overall_rmse_sd = np.std(overall_rmse_list)\n",
    "abundance_r_score_sd = np.std(abundance_r_score_list)\n",
    "abundance_mae_sd = np.std(abundance_mae_list)\n",
    "abundance_rmse_sd = np.std(abundance_rmse_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export threshold and performance metrics as a table\n",
    "metrics_dataframe = pd.DataFrame({'threshold':threshold_list,\n",
    "                                  'sensitivity':sensitivity_list,\n",
    "                                  'specificity':specificity_list,\n",
    "                                  'auc':auc_list,\n",
    "                                  'accuracy':accuracy_list,\n",
    "                                  'overall_r_score':overall_r_score_list,\n",
    "                                  'overall_mae':overall_mae_list,\n",
    "                                  'overall_rmse':overall_rmse_list,\n",
    "                                  'abundance_r_score':abundance_r_score_list,\n",
    "                                  'abundance_mae':abundance_mae_list,\n",
    "                                  'abundance_rmse':abundance_rmse_list})\n",
    "metrics_dataframe.to_csv(metrics_file, header=True, index=False, sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a text file to store the presence-absence conversion threshold\n",
    "file = open(threshold_file, 'w')\n",
    "file.write(str(round(threshold_mean, 5)))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write html text file\n",
    "output_report = os.path.join(output_folder, output_report_name)\n",
    "output_text = os.path.splitext(output_report)[0] + \".txt\"\n",
    "text_file = open(output_text, \"w\")\n",
    "text_file.write(\"<html>\\n\")\n",
    "text_file.write(\"<head>\\n\")\n",
    "text_file.write(\"<meta http-equiv=\\\"pragma\\\" content=\\\"no-cache\\\">\\n\")\n",
    "text_file.write(\"<meta http-equiv=\\\"Expires\\\" content=\\\"-1\\\">\\n\")\n",
    "text_file.write(\"</head>\\n\")\n",
    "text_file.write(\"<body>\\n\")\n",
    "text_file.write(\"<div style=\\\"width:90%;max-width:1000px;margin-left:auto;margin-right:auto\\\">\\n\")\n",
    "text_file.write(\"<h1 style=\\\"text-align:center;\\\">Distribution-abundance model performance for \" + taxon_name + \"</h1>\\n\")\n",
    "text_file.write(\"<br>\" + \"\\n\")\n",
    "text_file.write(\"<h2>Predicted Distribution-abundance Pattern</h2>\\n\")\n",
    "text_file.write(\"<p>Distribution-abundance was predicted by a composite hierarchical model: 1. a classifier predicted species presence or absence, and 2. a regressor predicted species proportional abundance in areas where the classifier predicted the species is present. The map below shows the output raster prediction.</p>\\n\")\n",
    "text_file.write(\"<p><i>Prediction step has not yet been performed. No output to display.</i></p>\\n\")\n",
    "text_file.write(\"<h2>Recursive Feature Elimination</h2>\\n\")\n",
    "text_file.write(\"<p>A total of 83 features were prepared for the modeling process. Given the individual responses of species to environmental gradients, the variation of each species distribution and abundance is likely best explained by species-specific feature combinations. Uninformative features are a potential source of noise in the models and retaining the uninformative features could result in reduced predictive performance. We selected the best performing set of features by recursive feature elimination based on the cross validation of ROC AUC for classification and R Squared for regression.</p>\\n\")\n",
    "text_file.write(\"<h3>Classifier Feature Elimination</h3>\\n\")\n",
    "text_file.write(\"<p>The optimal number of variables for the classifier was \" + str(recursive_eliminator_classifier.n_features_) + \".</p>\\n\")\n",
    "text_file.write(\"<a target='_blank' href='plots\\\\classifier_feature_elimination.png'><img style='display:inline-block;max-width:720px;width:100%;' src='plots\\\\classifier_feature_elimination.png'></a>\\n\")\n",
    "text_file.write(\"<h3>Regressor Feature Elimination</h3>\\n\")\n",
    "text_file.write(\"<p>The optimal number of variables for the regressor was \" + str(recursive_eliminator_regressor.n_features_) + \".</p>\\n\")\n",
    "text_file.write(\"<a target='_blank' href='plots\\\\regressor_feature_elimination.png'><img style='display:inline-block;max-width:720px;width:100%;' src='plots\\\\regressor_feature_elimination.png'></a>\\n\")\n",
    "text_file.write(\"<h2>Bayesian Optimization of Hyperparameters</h2>\\n\")\n",
    "text_file.write(\"<p>The hyperparameters of the classifier and regressor were independently optimized in a bayesian framework using a Gaussian Process as the generative model. Optimization performance was determined by 5-fold cross validation using area under the receiver operating characteristic curve (AUC) as the metric to maximize for the classifier and negative mean squared error (-MSE) as the metric to maximize for the regressor. 1000 optimization iterations were performed and the best set of parameters was selected based on the maximization criteria.</p>\\n\")\n",
    "text_file.write(\"<h3>Classifier Optimization</h3>\\n\")\n",
    "text_file.write(\"<p>The hyperparameters of the gradient boosting classifier (using the XGBoost implementation) were optimized to the following values:</p>\\n\")\n",
    "text_file.write(\"<p>max_depth = \" + str(int(classifier_parameters[0])) + \"</p>\\n\")\n",
    "text_file.write(\"<p>learning_rate = \" + str(classifier_parameters[1]) + \"</p>\\n\")\n",
    "text_file.write(\"<p>n_estimators = \" + str(int(classifier_parameters[2])) + \"</p>\\n\")\n",
    "text_file.write(\"<p>silent = True</p>\" + \"\\n\")\n",
    "text_file.write(\"<p>objective = 'binary:logistic'</p>\\n\")\n",
    "text_file.write(\"<p>booster = 'gbtree'</p>\\n\")\n",
    "text_file.write(\"<p>n_jobs = 1</p>\\n\")\n",
    "text_file.write(\"<p>gamma = \" + str(classifier_parameters[3]) + \"</p>\\n\")\n",
    "text_file.write(\"<p>min_child_weight = \" + str(int(classifier_parameters[4])) + \"</p>\\n\")\n",
    "text_file.write(\"<p>max_delta_step = \" + str(int(classifier_parameters[5])) + \"</p>\\n\")\n",
    "text_file.write(\"<p>subsample = \" + str(classifier_parameters[6]) + \"</p>\\n\")\n",
    "text_file.write(\"<p>colsample_bytree = \" + str(classifier_parameters[7]) + \"</p>\\n\")\n",
    "text_file.write(\"<p>colsample_bylevel = \" + str(classifier_parameters[8]) + \"</p>\\n\")\n",
    "text_file.write(\"<p>reg_alpha = \" + str(classifier_parameters[9]) + \"</p>\\n\")\n",
    "text_file.write(\"<p>reg_lambda = \" + str(classifier_parameters[10]) + \"</p>\\n\")\n",
    "text_file.write(\"<p>scale_pos_weight = \" + str(classifier_parameters[11]) + \"</p>\\n\")\n",
    "text_file.write(\"<a target='_blank' href='plots\\\\convergence_classifier.png'><img style='display:inline-block;max-width:720px;width:100%;' src='plots\\\\convergence_classifier.png'></a>\\n\")\n",
    "text_file.write(\"<h3>Regressor Optimization</h3>\\n\")\n",
    "text_file.write(\"<p>The hyperparameters of the gradient boosting regressor (using the XGBoost implementation) were optimized to the following values:</p>\\n\")\n",
    "text_file.write(\"<p>max_depth = \" + str(int(regressor_parameters[0])) + \"</p>\\n\")\n",
    "text_file.write(\"<p>learning_rate = \" + str(regressor_parameters[1]) + \"</p>\\n\")\n",
    "text_file.write(\"<p>n_estimators = \" + str(int(regressor_parameters[2])) + \"</p>\\n\")\n",
    "text_file.write(\"<p>silent = True</p>\\n\")\n",
    "text_file.write(\"<p>objective = 'reg:linear'</p>\\n\")\n",
    "text_file.write(\"<p>booster = 'gbtree'</p>\\n\")\n",
    "text_file.write(\"<p>n_jobs = 1</p>\\n\")\n",
    "text_file.write(\"<p>gamma = \" + str(regressor_parameters[3]) + \"</p>\\n\")\n",
    "text_file.write(\"<p>min_child_weight = \" + str(int(regressor_parameters[4])) + \"</p>\\n\")\n",
    "text_file.write(\"<p>max_delta_step = \" + str(int(regressor_parameters[5])) + \"</p>\\n\")\n",
    "text_file.write(\"<p>subsample = \" + str(regressor_parameters[6]) + \"</p>\\n\")\n",
    "text_file.write(\"<p>colsample_bytree = \" + str(regressor_parameters[7]) + \"</p>\\n\")\n",
    "text_file.write(\"<p>colsample_bylevel = \" + str(regressor_parameters[8]) + \"</p>\\n\")\n",
    "text_file.write(\"<p>reg_alpha = \" + str(regressor_parameters[9]) + \"</p>\\n\")\n",
    "text_file.write(\"<p>reg_lambda = \" + str(regressor_parameters[10]) + \"</p>\\n\")\n",
    "text_file.write(\"<p>scale_pos_weight = \" + str(regressor_parameters[11]) + \"</p>\\n\")\n",
    "text_file.write(\"<a target='_blank' href='plots\\\\convergence_regressor.png'><img style='display:inline-block;max-width:720px;width:100%;' src='plots\\\\convergence_regressor.png'></a>\\n\")\n",
    "text_file.write(\"<h2>Composite Model Performance</h2>\\n\")\n",
    "text_file.write(\"<p>Model performance was measured by pseudo r squared, mean absolute error, and root mean squared error for the composite prediction. Additionally, the performance of the absence class is reported as an area under the receiver operating characteristic curve (AUC) and overall accuracy, where specificity and sensitivity are as close to equal as possible (i.e, the model performs equally well at predicting absences and presences). All performance results are reported as the mean and standard deviation of the independent test predictions from 100 train-test splits. For each split, 30% of the data was withheld from model training and used as an independent test partition. The test partition was randomly selected from cover class strata: 0%, 1-10%, 11-25%, and 26-100%.</p>\\n\")\n",
    "text_file.write(\"<h3>Overall Performance</h3>\\n\")\n",
    "text_file.write(\"<p>R Squared = \" + str(np.round(overall_r_score_mean, 2)) + \" +/- \" + str(np.round(overall_r_score_sd, 2)) +\"</p>\\n\")\n",
    "text_file.write(\"<p>Mean Absolute Error = \" + str(np.round(overall_mae_mean, 2)) + \" +/- \" + str(np.round(overall_mae_sd, 2)) +\"</p>\\n\")\n",
    "text_file.write(\"<p>Root Mean Squared Error = \" + str(np.round(overall_rmse_mean, 2)) + \" +/- \" + str(np.round(overall_rmse_sd, 2)) +\"</p>\\n\")\n",
    "text_file.write(\"<h3>Absence Performance</h3>\\n\")\n",
    "text_file.write(\"<p>AUC = \" + str(np.round(auc_mean, 2)) + \" +/- \" + str(np.round(auc_sd, 2)) +\"</p>\\n\")\n",
    "text_file.write(\"<p>Presence-Absence Accuracy = \" + str(np.round(accuracy_mean, 2)) + \" +/- \" + str(np.round(accuracy_sd, 2)) +\"</p>\\n\")\n",
    "text_file.write(\"<h3>Abundance Performance</h3>\\n\")\n",
    "text_file.write(\"<p>R Squared = \" + str(np.round(abundance_r_score_mean, 2)) + \" +/- \" + str(np.round(abundance_r_score_sd, 2)) +\"</p>\\n\")\n",
    "text_file.write(\"<p>Mean Absolute Error = \" + str(np.round(abundance_mae_mean, 2)) + \" +/- \" + str(np.round(abundance_mae_sd, 2)) +\"</p>\\n\")\n",
    "text_file.write(\"<p>Root Mean Squared Error = \" + str(np.round(abundance_rmse_mean, 2)) + \" +/- \" + str(np.round(abundance_rmse_sd, 2)) +\"</p>\\n\")\n",
    "text_file.write(\"<h3>Classifier Importances</h3>\\n\")\n",
    "text_file.write(\"<p>The Variable Importance plot for the classifier is shown below:</p>\\n\")\n",
    "text_file.write(\"<a target='_blank' href='plots\\\\importance_classifier.png'><img style='display:inline-block;max-width:1000px;width:100%;' src='plots\\\\importance_classifier.png'></a>\\n\")\n",
    "text_file.write(\"<h3>Regressor Importances</h3>\\n\")\n",
    "text_file.write(\"<p>The Variable Importance plot for the regressor is shown below:</p>\\n\")\n",
    "text_file.write(\"<a target='_blank' href='plots\\\\importance_regressor.png'><img style='display:inline-block;max-width:1000px;width:100%;' src='plots\\\\importance_regressor.png'></a>\\n\")\n",
    "text_file.write(r\"<h2>Variable Correlation</h2>\" + \"\\n\")\n",
    "text_file.write(\"<p>The plot below explores variable correlation. No attempt was made to remove highly correlated variables (shown in the plot dark blue).</p>\\n\")\n",
    "text_file.write(\"<a target='_blank' href='plots\\\\variable_correlation.png'><img style='display:inline-block;width:100%;' src='plots\\\\variable_correlation.png'></a>\\n\")\n",
    "text_file.write(\"</div>\\n\")\n",
    "text_file.write(\"</body>\\n\")\n",
    "text_file.write(\"</html>\\n\")\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename HTML Text to HTML\n",
    "if os.path.exists(output_report) == True:\n",
    "    os.remove(output_report)\n",
    "os.rename(output_text, output_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
