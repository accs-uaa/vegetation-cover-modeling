{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# ---------------------------------------------------------------------------\n",
    "# Classifiers Train and Test\n",
    "# Author: Timm Nawrocki, Alaska Center for Conservation Science\n",
    "# Created on: 2018-08-18\n",
    "# Usage: Must be executed as a Jupyter Notebook in an Anaconda 3 installation on a Google Cloud virtual machine with 64 vCPUs and 57.6 GB of CPU memory with an Ubuntu operating system (18.04 LTS).\n",
    "# Description: \"Classifiers Train and Test\" trains a classification model to determine cover values of 0% from cover values greater than or equal to 1% using the presence and absence data in the training dataset. Subsequently, the training dataset is subsetted to include only the presence data. Two additional classifiers are trained to distinguish 1-10% from 11-100% and 1-25% from 26-100%.\n",
    "# ---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script runs the model train and test steps to output a model performance and variable importance report and classifier and threshold files that can be transferred to the predict script. The script is formatted as a Jupyter Notebook and is intended to be run on a Google Cloud virtual machine with 64 vCPUs and 57.6 GB of CPU memory with an Ubuntu operating system (18.04 LTS). The Random Forest classifier in this script is set to use 16 cores and may work inefficiently or not at all on a machine that has less than 64 cores. For information on generating inputs for this script or on setting up Google Cloud virtual machines, see the [project readme](https://github.com/accs-uaa/vegetation-cover-modeling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plot\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.externals import joblib\n",
    "print('All modules successfully imported.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define user input variables\n",
    "print('Enter root directory:')\n",
    "root_folder = input()\n",
    "print('Enter name of output folder:')\n",
    "output_folder = input()\n",
    "print('Enter name of input data csv file:')\n",
    "input_data_name = input()\n",
    "print('Enter name of output report file:')\n",
    "output_report_name = input()\n",
    "print('Enter name of taxon:')\n",
    "taxon_name = input()\n",
    "print('All user-defined variables input.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variable sets\n",
    "predictor_all = ['compoundTopographic', 'dateFreeze_2000s', 'dateThaw_2000s', 'elevation', 'floodplainsDist', 'growingSeason_2000s', 'heatLoad', 'integratedMoisture', 'precipAnnual_2000s', 'roughness', 'siteExposure', 'slope', 'streamLargeDist', 'streamSmallDist', 'summerWarmth_2000s', 'surfaceArea', 'surfaceRelief', 'aspect', 'may_1_ultraBlue', 'may_2_blue', 'may_3_green', 'may_4_red', 'may_5_nearInfrared', 'may_6_shortInfrared1', 'may_7_shortInfrared2', 'may_evi2', 'may_nbr', 'may_ndmi', 'may_ndsi', 'may_ndvi', 'may_ndwi', 'june_1_ultraBlue', 'june_2_blue', 'june_3_green', 'june_4_red', 'june_5_nearInfrared', 'june_6_shortInfrared1', 'june_7_shortInfrared2', 'june_evi2', 'june_nbr', 'june_ndmi', 'june_ndsi', 'june_ndvi', 'june_ndwi', 'july_1_ultraBlue', 'july_2_blue', 'july_3_green', 'july_4_red', 'july_5_nearInfrared', 'july_6_shortInfrared1', 'july_7_shortInfrared2', 'july_evi2', 'july_nbr', 'july_ndmi', 'july_ndsi', 'july_ndvi', 'july_ndwi', 'august_1_ultraBlue', 'august_2_blue', 'august_3_green', 'august_4_red', 'august_5_nearInfrared', 'august_6_shortInfrared1', 'august_7_shortInfrared2', 'august_evi2', 'august_nbr', 'august_ndmi', 'august_ndsi', 'august_ndvi', 'august_ndwi', 'september_1_ultraBlue', 'september_2_blue', 'september_3_green', 'september_4_red', 'september_5_nearInfrared', 'september_6_shortInfrared1', 'september_7_shortInfrared2', 'september_evi2', 'september_nbr', 'september_ndmi', 'september_ndsi', 'september_ndvi', 'september_ndwi']\n",
    "predictor_metrics = ['compoundTopographic', 'dateFreeze_2000s', 'dateThaw_2000s', 'elevation', 'floodplainsDist', 'growingSeason_2000s', 'heatLoad', 'integratedMoisture', 'precipAnnual_2000s', 'roughness', 'siteExposure', 'slope', 'streamLargeDist', 'streamSmallDist', 'summerWarmth_2000s', 'surfaceArea', 'surfaceRelief', 'aspect', 'may_2_blue', 'may_evi2', 'may_nbr', 'may_ndmi', 'may_ndsi', 'may_ndvi', 'may_ndwi', 'june_2_blue', 'june_evi2', 'june_nbr', 'june_ndmi', 'june_ndsi', 'june_ndvi', 'june_ndwi', 'july_2_blue', 'july_evi2', 'july_nbr', 'july_ndmi', 'july_ndsi', 'july_ndvi', 'july_ndwi', 'august_2_blue', 'august_evi2', 'august_nbr', 'august_ndmi', 'august_ndsi', 'august_ndvi', 'august_ndwi', 'september_2_blue', 'september_evi2', 'september_nbr', 'september_ndmi', 'september_ndsi', 'september_ndvi', 'september_ndwi']\n",
    "predictor_midsummer = ['compoundTopographic', 'dateFreeze_2000s', 'dateThaw_2000s', 'elevation', 'floodplainsDist', 'growingSeason_2000s', 'heatLoad', 'integratedMoisture', 'precipAnnual_2000s', 'roughness', 'siteExposure', 'slope', 'streamLargeDist', 'streamSmallDist', 'summerWarmth_2000s', 'surfaceArea', 'surfaceRelief', 'aspect', 'july_1_ultraBlue', 'july_2_blue', 'july_3_green', 'july_4_red', 'july_5_nearInfrared', 'july_6_shortInfrared1', 'july_7_shortInfrared2', 'july_evi2', 'july_nbr', 'july_ndmi', 'july_ndsi', 'july_ndvi', 'july_ndwi']\n",
    "predictor_raw = ['compoundTopographic', 'dateFreeze_2000s', 'dateThaw_2000s', 'elevation', 'floodplainsDist', 'growingSeason_2000s', 'heatLoad', 'integratedMoisture', 'precipAnnual_2000s', 'roughness', 'siteExposure', 'slope', 'streamLargeDist', 'streamSmallDist', 'summerWarmth_2000s', 'surfaceArea', 'surfaceRelief', 'aspect', 'may_1_ultraBlue', 'may_2_blue', 'may_3_green', 'may_4_red', 'may_5_nearInfrared', 'may_6_shortInfrared1', 'may_7_shortInfrared2', 'june_1_ultraBlue', 'june_2_blue', 'june_3_green', 'june_4_red', 'june_5_nearInfrared', 'june_6_shortInfrared1', 'june_7_shortInfrared2', 'july_1_ultraBlue', 'july_2_blue', 'july_3_green', 'july_4_red', 'july_5_nearInfrared', 'july_6_shortInfrared1', 'july_7_shortInfrared2', 'august_1_ultraBlue', 'august_2_blue', 'august_3_green', 'august_4_red', 'august_5_nearInfrared', 'august_6_shortInfrared1', 'august_7_shortInfrared2', 'september_1_ultraBlue', 'september_2_blue', 'september_3_green', 'september_4_red', 'september_5_nearInfrared', 'september_6_shortInfrared1', 'september_7_shortInfrared2']\n",
    "zero_variable = ['zero']\n",
    "ten_variable = ['ten']\n",
    "twentyfive_variable = ['twentyfive']\n",
    "retain_variables = ['cover', 'project', 'siteID', 'siteCode', 'methodSurvey', 'methodCover', 'strata']\n",
    "coordinates = ['POINT_X', 'POINT_Y']\n",
    "all_variables = predictor_all + zero_variable + ten_variable + twentyfive_variable + retain_variables + coordinates\n",
    "print('Variable sets loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot Pearson correlation of predictor variables\n",
    "def plotVariableCorrelation(X_train, outFile):\n",
    "    # Calculate Pearson correlation coefficient between the predictor variables, where -1 is perfect negative correlation and 1 is perfect positive correlation\n",
    "    correlation = X_train.astype('float64').corr()\n",
    "    # Generate a mask for the upper triangle of plot\n",
    "    mask = np.zeros_like(correlation, dtype=np.bool)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "    # Set up the matplotlib figure\n",
    "    f, ax = plot.subplots(figsize=(11, 9))\n",
    "    # Generate a custom diverging colormap\n",
    "    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "    # Draw the heatmap with the mask and correct aspect ratio\n",
    "    correlation_plot = sns.heatmap(correlation, mask=mask, cmap=cmap, vmax=.3, center=0, square=True, linewidths=.5, cbar_kws={'shrink': .5})\n",
    "    correlation_figure = correlation_plot.get_figure()\n",
    "    correlation_figure.savefig(outFile, bbox_inches='tight', dpi=300)\n",
    "    # Clear plot workspace\n",
    "    plot.clf()\n",
    "    plot.close()\n",
    "\n",
    "print('Function \"plotVariableCorrelation\" loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot variable importances\n",
    "def plotVariableImportances(inModel, x_train, outVariableFile):\n",
    "    # Get numerical feature importances\n",
    "    importances = list(inModel.feature_importances_)\n",
    "    # List of tuples with variable and importance\n",
    "    feature_list = list(x_train.columns)\n",
    "    feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "    # Sort the feature importances by most important first\n",
    "    feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "    # Initialize the plot and set figure size\n",
    "    variable_figure = plot.figure()\n",
    "    plot.style.use('fivethirtyeight')\n",
    "    fig_size = plot.rcParams[\"figure.figsize\"]\n",
    "    fig_size[0] = 36\n",
    "    fig_size[1] = 12\n",
    "    plot.rcParams[\"figure.figsize\"] = fig_size\n",
    "    # Create list of x locations for plotting\n",
    "    x_values = list(range(len(importances)))\n",
    "    # Make a bar chart of the variable importances\n",
    "    plot.bar(x_values, importances, orientation = 'vertical')\n",
    "    # Tick labels for x axis\n",
    "    plot.xticks(x_values, feature_list, rotation='vertical')\n",
    "    # Axis labels and title\n",
    "    plot.ylabel('Importance'); plot.xlabel('Variable'); plot.title('Variable Importances');\n",
    "    # Export\n",
    "    variable_figure.savefig(outVariableFile, bbox_inches=\"tight\", dpi=300)\n",
    "    # Clear plot workspace\n",
    "    plot.clf()\n",
    "    plot.close()\n",
    "    \n",
    "print('Function \"plotVariableImportances\" loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate performance metrics based on a specified threshold value\n",
    "def thresholdMetrics(inIndex, inProbability, inValue, y_test):\n",
    "    outThresholded = np.zeros(inIndex.shape)\n",
    "    outThresholded[inIndex > inValue] = 1\n",
    "    confusion_test = confusion_matrix(y_test, outThresholded)\n",
    "    true_negative = confusion_test[0,0]\n",
    "    false_negative = confusion_test[1,0]\n",
    "    true_positive = confusion_test[1,1]\n",
    "    false_positive = confusion_test[0,1]\n",
    "    outSensitivity = true_positive / (true_positive + false_negative)\n",
    "    outSpecificity = true_negative / (true_negative + false_positive)\n",
    "    outAUC = roc_auc_score(y_test, inProbability)\n",
    "    outAccuracy = (true_negative + true_positive) / (true_negative + false_positive + false_negative + true_positive)\n",
    "    return (outThresholded, outSensitivity, outSpecificity, outAUC, outAccuracy)\n",
    "\n",
    "print('Function \"thresholdMetrics\" loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to fit a classifier using training data and determine a best classification threshold using the test data\n",
    "def trainTestModel(X_train, y_train, X_test, y_test, variable):\n",
    "    # Fit a random forest classifier to the training dataset\n",
    "    rf_classify = RandomForestClassifier(n_estimators = 5000, criterion='entropy', max_features='log2', bootstrap = True, oob_score = True, n_jobs=1, class_weight = \"balanced\")\n",
    "    rf_classify.fit(X_train, y_train)\n",
    "    # Use the random forest classifier to predict probabilities for the test dataset\n",
    "    test_prediction = rf_classify.predict_proba(X_test)\n",
    "    # Convert the positive class probabilities to a list of probabilities\n",
    "    test_probability = [p[1] for p in test_prediction]\n",
    "    # Convert the postitive class probabilities to an index between 0 and 1000\n",
    "    test_index = [int((p[1] * 1000) + 0.5) for p in test_prediction]\n",
    "    # Iterate through numbers between 0 and 1000 to output a list of sensitivity and specificity values per threshold number\n",
    "    i = 1\n",
    "    test_index = np.asarray(test_index)\n",
    "    sensitivity_list = []\n",
    "    specificity_list = []\n",
    "    while i < 1001:\n",
    "        test_thresholded, sensitivity_test, specificity_test, auc_test, accuracy_test = thresholdMetrics(test_index, test_probability, i, y_test)\n",
    "        sensitivity_list.append(sensitivity_test)\n",
    "        specificity_list.append(specificity_test)\n",
    "        i = i + 1\n",
    "    # Calculate a list of absolute value of difference between sensitivity and specificity and find the optimal threshold\n",
    "    difference_list = [a - b for a, b in zip(sensitivity_list, specificity_list)]\n",
    "    value, threshold = min((value, threshold) for (threshold, value) in enumerate(difference_list) if value >= 0)\n",
    "    # Calculate the prediction index to a binary 0 or 1 output using the optimal threshold\n",
    "    test_thresholded, sensitivity_test, specificity_test, auc_test, accuracy_test = thresholdMetrics(test_index, test_probability, threshold, y_test)\n",
    "    return [threshold, sensitivity_test, specificity_test, auc_test, accuracy_test, rf_classify.oob_score_]\n",
    "\n",
    "print('Function \"trainTestModel\" loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to cross-validate a classifier using 100 stratified shuffle splits\n",
    "def crossValidateModel(inDF, predictors, response):\n",
    "    # Define the predictor labels (X) and the response label (y) in the input dataframe\n",
    "    X = inDF[predictors]\n",
    "    y = np.array(inDF[response]).ravel()\n",
    "    strata = np.array(inDF['strata']).ravel()\n",
    "    # Create empty lists to store the results of successive test runs\n",
    "    threshold_list = []\n",
    "    sensitivity_list = []\n",
    "    specificity_list = []\n",
    "    auc_list = []\n",
    "    accuracy_list = []\n",
    "    oob_score_list = []\n",
    "    # Conduct a classification run for each training-test split\n",
    "    i = 1\n",
    "    while i < 101:\n",
    "        # Define the training and test partitions\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, train_size = 0.7, random_state = None, shuffle = True, stratify = strata)\n",
    "        # Train and test a classifier and output threshold and performance metrics\n",
    "        threshold, sensitivity, specificity, auc, accuracy, oob_score = trainTestModel(X_train, y_train, X_test, y_test, zero_variable)\n",
    "        # Append threshold and performance metrics to lists\n",
    "        threshold_list.append(threshold)\n",
    "        sensitivity_list.append(sensitivity)\n",
    "        specificity_list.append(specificity)\n",
    "        auc_list.append(auc)\n",
    "        accuracy_list.append(accuracy)\n",
    "        oob_score_list.append(oob_score)\n",
    "        print('Model iteration ' + str(i) + ' out of 100 trained and tested...')\n",
    "        i = i + 1\n",
    "    return [threshold_list, sensitivity_list, specificity_list, auc_list, accuracy_list, oob_score_list]\n",
    "\n",
    "print('Function \"crossValidateModel\" loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to train and export a final classifier\n",
    "def trainExportClassifier(inDF, predictors, response, outModel, outImportance):\n",
    "    # Define the predictor labels (X) and the response label (y) in the input dataframe\n",
    "    X = inDF[predictors]\n",
    "    y = np.array(inDF[response]).ravel()\n",
    "    # Fit a classifier to the input dataset\n",
    "    rf_classify = RandomForestClassifier(n_estimators = 5000, bootstrap = True, oob_score = True, n_jobs=16, class_weight = \"balanced\")\n",
    "    rf_classify.fit(X, y)\n",
    "    # Save classifier to an external file\n",
    "    joblib.dump(rf_classify, outModel)\n",
    "    # Export a variable importance plot\n",
    "    plotVariableImportances(rf_classify, X, outImportance)\n",
    "    print('Final model trained and exported...')\n",
    "    \n",
    "print('Function \"trainExportClassifier\" loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to output threshold to text file\n",
    "def thresholdOut(inThresholdList, outThresholdFile):\n",
    "    file = open(outThresholdFile, 'w')\n",
    "    file.write(str(int(np.mean(inThresholdList) + 0.5)))\n",
    "    file.close()\n",
    "\n",
    "print('Function \"thresholdOut\" loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import input data from csv file\n",
    "input_file = os.path.join(os.path.join(root_folder, \"speciesData\"), input_data_name)\n",
    "input_df = pd.read_csv(input_file)\n",
    "# Convert numerical data to integers\n",
    "input_df[predictor_all + zero_variable + ten_variable + twentyfive_variable + ['cover'] + ['strata']] = input_df[predictor_all + zero_variable + ten_variable + twentyfive_variable + ['cover'] + ['strata']].astype(int)\n",
    "print(input_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a plots folder if it does not exist\n",
    "plots_folder = os.path.join(output_folder, \"plots\")\n",
    "if not os.path.exists(plots_folder):\n",
    "    os.makedirs(plots_folder)\n",
    "    print('Plots folder created.')\n",
    "else:\n",
    "    print('Plots folder already exists.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set initial plot sizefig_size = plot.rcParams[\"figure.figsize\"]\n",
    "fig_size = plot.rcParams[\"figure.figsize\"]\n",
    "fig_size[0] = 16\n",
    "fig_size[1] = 12\n",
    "plot.rcParams[\"figure.figsize\"] = fig_size\n",
    "print('Plot size parameters configured.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export a Pearson Correlation plot for the predictor variables\n",
    "variableCorrelation = os.path.join(plots_folder, \"variableCorrelation.png\")\n",
    "plotVariableCorrelation(input_df[predictor_all], variableCorrelation)\n",
    "print('Pearson Correlation plot saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set initial plot sizefig_size = plot.rcParams[\"figure.figsize\"]\n",
    "fig_size = plot.rcParams[\"figure.figsize\"]\n",
    "fig_size[0] = 36\n",
    "fig_size[1] = 12\n",
    "plot.rcParams[\"figure.figsize\"] = fig_size\n",
    "print('Plot size parameters configured.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 100 train and test iterations of zero classifier\n",
    "threshold_list_0, sensitivity_list_0, specificity_list_0, auc_list_0, accuracy_list_0, oob_score_list_0 = crossValidateModel(input_df, predictor_all, zero_variable)\n",
    "\n",
    "# Train and export a final model using the full input data\n",
    "model_0 = os.path.join(output_folder, 'classifier_0.joblib')\n",
    "variableImportance_0 = os.path.join(plots_folder, 'variableImportance_0.png')\n",
    "trainExportClassifier(input_df, predictor_all, zero_variable, model_0, variableImportance_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(auc_list_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset the input dataframe to include only the presence data\n",
    "subset_df = input_df[input_df['strata'] >= 1]\n",
    "print(subset_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 100 train and test iterations of ten classifier\n",
    "threshold_list_10, sensitivity_list_10, specificity_list_10, auc_list_10, accuracy_list_10, oob_score_list_10 = crossValidateModel(subset_df, predictor_all, ten_variable)\n",
    "\n",
    "# Train and export a final model using the full input data\n",
    "model_10 = os.path.join(output_folder, 'classifier_10.joblib')\n",
    "variableImportance_10 = os.path.join(plots_folder, 'variableImportance_10.png')\n",
    "trainExportClassifier(subset_df, predictor_all, ten_variable, model_10, variableImportance_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(auc_list_10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 100 train and test iterations of twentyfive classifier\n",
    "threshold_list_25, sensitivity_list_25, specificity_list_25, auc_list_25, accuracy_list_25, oob_score_list_25 = crossValidateModel(subset_df, predictor_all, twentyfive_variable)\n",
    "\n",
    "# Train and export a final model using the full input data\n",
    "model_25 = os.path.join(output_folder, 'classifier_25.joblib')\n",
    "variableImportance_25 = os.path.join(plots_folder, 'variableImportance_25.png')\n",
    "trainExportClassifier(subset_df, predictor_all, twentyfive_variable, model_25, variableImportance_25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(auc_list_25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a text file for each classifier containing the threshold value that minimizes the absolute value difference between specificity and sensitivity\n",
    "thresholdOut(threshold_list_0, os.path.join(output_folder, 'threshold_0.txt'))\n",
    "thresholdOut(threshold_list_10, os.path.join(output_folder, 'threshold_10.txt'))\n",
    "thresholdOut(threshold_list_25, os.path.join(output_folder, 'threshold_25.txt'))\n",
    "print('Threshold files saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write html text file\n",
    "output_report = os.path.join(output_folder, output_report_name)\n",
    "output_text = os.path.splitext(output_report)[0] + \".txt\"\n",
    "text_file = open(output_text, \"w\")\n",
    "text_file.write(\"<html>\\n\")\n",
    "text_file.write(\"<head>\\n\")\n",
    "text_file.write(\"<meta http-equiv=\\\"pragma\\\" content=\\\"no-cache\\\">\\n\")\n",
    "text_file.write(\"<meta http-equiv=\\\"Expires\\\" content=\\\"-1\\\">\\n\")\n",
    "text_file.write(\"</head>\\n\")\n",
    "text_file.write(\"<body>\\n\")\n",
    "text_file.write(\"<div style=\\\"width:90%;max-width:1000px;margin-left:auto;margin-right:auto\\\">\\n\")\n",
    "text_file.write(\"<h1 style=\\\"text-align:center;\\\">Classified Cover Modeling Performance for \" + taxon_name + \"</h1>\\n\")\n",
    "text_file.write(r\"<br>\" + \"\\n\")\n",
    "text_file.write(r\"<h2>Model Performance</h2>\" + \"\\n\")\n",
    "text_file.write(\"<p>Model performance is measured by sensitivity, specificity, accuracy, and area under curve (auc) for each of the model component classifiers as calculated by averaging 100 iterations of stratified random train-test splits. Each component is a binary classifier that distinguishes between a break in cover. The breaks are coded as follows: the '0' classifier distinguishes cover values greater than 0%, the '10' classifier distinguishes between cover values greater than 10%, and the '25' classifier distinguishes between cover values greater than 25%. Each component was trained separately and a threshold that minimized the absolute value difference between sensitivity and specificity where sensitivity was greater than specificity was selected against 100 random stratitified independent partitions of test data. All model metrics except the bootstrap are relative to the independent partitions of test data.</p>\\n\")\n",
    "text_file.write(r\"<h3>Performance of '0' Classifier</h3>\" + \"\\n\")\n",
    "text_file.write(\"<p><b>Sensitivity</b> of the '0' Classifier is <b>\" + str(round(np.mean(sensitivity_list_0), 3)) + \"</b></p>\\n\")\n",
    "text_file.write(\"<p><b>Specificity</b> of the '0' Classifier is <b>\" + str(round(np.mean(specificity_list_0), 3)) + \"</b></p>\\n\")\n",
    "text_file.write(\"<p>Overall <b>Accuracy</b> of the '0' Classifier is <b>\" + str(round(np.mean(accuracy_list_0), 3)) + \"</b></p>\\n\")\n",
    "text_file.write(\"<p>The '0' Classifier <b>Out Of Bag Score</b> is <b>\" + str(round(np.mean(oob_score_list_0), 3)) + \"</b></p>\\n\")\n",
    "text_file.write(\"<p><b>AUC value</b> of the '0' Classifier is <b>\" + str(round(np.mean(auc_list_0), 3)) + \"</b></p>\\n\")\n",
    "text_file.write(\"<p>The Variable Importance plot for the '0' Classifier is shown below:</p>\\n\")\n",
    "text_file.write(\"<a target='_blank' href='plots\\\\variableImportance_0.png'><img style='display:inline-block;max-width:1000px;width:100%;' src='plots\\\\variableImportance_0.png'></a>\\n\")\n",
    "text_file.write(r\"<h3>Performance of '10' Classifier</h3>\" + \"\\n\")\n",
    "text_file.write(\"<p><b>Sensitivity</b> of the '10' Classifier is <b>\" + str(round(np.mean(sensitivity_list_10), 3)) + \"</b></p>\\n\")\n",
    "text_file.write(\"<p><b>Specificity</b> of the '10' Classifier is <b>\" + str(round(np.mean(specificity_list_10), 3)) + \"</b></p>\\n\")\n",
    "text_file.write(\"<p>Overall <b>Accuracy</b> of the '10' Classifier is <b>\" + str(round(np.mean(accuracy_list_10), 3)) + \"</b></p>\\n\")\n",
    "text_file.write(\"<p>The '10' Classifier <b>Out Of Bag Score</b> is <b>\" + str(round(np.mean(oob_score_list_10), 3)) + \"</b></p>\\n\")\n",
    "text_file.write(\"<p><b>AUC value</b> of the '10' Classifier is <b>\" + str(round(np.mean(auc_list_10), 3)) + \"</b></p>\\n\")\n",
    "text_file.write(\"<p>The Variable Importances plot for the '10' Classifier is shown below:</p>\\n\")\n",
    "text_file.write(\"<a target='_blank' href='plots\\\\variableImportance_10.png'><img style='display:inline-block;max-width:1000px;width:100%;' src='plots\\\\variableImportance_10.png'></a>\\n\")\n",
    "text_file.write(r\"<h3>Performance of '25' Classifier</h3>\" + \"\\n\")\n",
    "text_file.write(\"<p><b>Sensitivity</b> of the '25' Classifier is <b>\" + str(round(np.mean(sensitivity_list_25), 3)) + \"</b></p>\\n\")\n",
    "text_file.write(\"<p><b>Specificity</b> of the '25' Classifier is <b>\" + str(round(np.mean(specificity_list_25), 3)) + \"</b></p>\\n\")\n",
    "text_file.write(\"<p>Overall <b>Accuracy</b> of the '25' Classifier is <b>\" + str(round(np.mean(accuracy_list_25), 3)) + \"</b></p>\\n\")\n",
    "text_file.write(\"<p>The '25' Classifier <b>Out Of Bag Score</b> is <b>\" + str(round(np.mean(oob_score_list_25), 3)) + \"</b></p>\\n\")\n",
    "text_file.write(\"<p><b>AUC value</b> of the '25' Classifier is <b>\" + str(round(np.mean(auc_list_25), 3)) + \"</b></p>\\n\")\n",
    "text_file.write(\"<p>The Variable Importances plot for the '25' Classifier is shown below:</p>\\n\")\n",
    "text_file.write(\"<a target='_blank' href='plots\\\\variableImportance_25.png'><img style='display:inline-block;max-width:1000px;width:100%;' src='plots\\\\variableImportance_25.png'></a>\\n\")\n",
    "text_file.write(r\"<h2>Variable Correlation</h2>\" + \"\\n\")\n",
    "text_file.write(\"<p>The plot below explores variable correlation. No attempt was made to remove highly correlated variables (shown in the plot dark blue).</p>\\n\")\n",
    "text_file.write(\"<a target='_blank' href='plots\\\\variableCorrelation.png'><img style='display:inline-block;width:100%;' src='plots\\\\variableCorrelation.png'></a>\\n\")\n",
    "text_file.write(\"</div>\\n\")\n",
    "text_file.write(\"</body>\\n\")\n",
    "text_file.write(\"</html>\\n\")\n",
    "text_file.close()\n",
    "\n",
    "# Rename HTML Text to HTML\n",
    "if os.path.exists(output_report) == True:\n",
    "    os.remove(output_report)\n",
    "os.rename(output_text, output_report)\n",
    "print('Report saved. Script complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
